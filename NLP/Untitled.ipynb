{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdadcea",
   "metadata": {},
   "source": [
    "# Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368469b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3eed25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='Hello Rosa'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regular expressions to recognize several different human greetings at the start of a conversation\n",
    "r = \"(hi|hello|hey)[ ]*([a-z]*)\"\n",
    "re.match(r,'Hello Rosa',flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b18fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 5), match='hi ho'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r,\"hi ho, hi ho, it's off to work...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101fc4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 5), match='hi ho'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r,\"hi ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09df71fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='hey'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r,\"hey! whats up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7348ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='hi asd'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r,\"hi asd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09713a",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a0302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the\n",
    "... age of 26.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde91948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01448ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.split(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5aab791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26., Jefferson, Monticello, Thomas, age, at, began, building, of, the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "', '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5bb1ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "one_hot_vectors = np.zeros((num_tokens,vocab_size),int)\n",
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb40aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26. Jefferson Monticello Thomas age at began building of the'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(token_sequence):\n",
    "    one_hot_vectors[i,vocab.index(word)] =1\n",
    "\" \".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "478179c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fefb4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fcc1e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26.</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Thomas</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the\n",
       "0    0          0           0       1    0   0      0         0   0    0\n",
       "1    0          1           0       0    0   0      0         0   0    0\n",
       "2    0          0           0       0    0   0      1         0   0    0\n",
       "3    0          0           0       0    0   0      0         1   0    0\n",
       "4    0          0           1       0    0   0      0         0   0    0\n",
       "5    0          0           0       0    0   1      0         0   0    0\n",
       "6    0          0           0       0    0   0      0         0   0    1\n",
       "7    0          0           0       0    1   0      0         0   0    0\n",
       "8    0          0           0       0    0   0      0         0   1    0\n",
       "9    1          0           0       0    0   0      0         0   0    0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(one_hot_vectors,columns=vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95c0faf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thomas': 1,\n",
       " 'Jefferson': 1,\n",
       " 'began': 1,\n",
       " 'building': 1,\n",
       " 'Monticello': 1,\n",
       " 'at': 1,\n",
       " 'the': 1,\n",
       " 'age': 1,\n",
       " 'of': 1,\n",
       " '26.': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bow = {}\n",
    "for word in sentence.split():\n",
    "    sentence_bow[word]=1\n",
    "sentence_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff051f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([token,1] for token in sentence.split())),columns=['sent']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9e90002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
       "sent       1          1      1         1           1   1    1    1   1    1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fe746b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the\\\n",
    "... age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and\\\n",
    "... carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece\\\n",
    "... was Jefferson's obsession.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4aafc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for i,sent in enumerate(sentences.split('\\n')):\n",
    "    corpus[f'sent{i}']=dict((tok,1)for tok in sent.split())\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c9e729c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>theage</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>...</th>\n",
       "      <th>South</th>\n",
       "      <th>Pavilion</th>\n",
       "      <th>in</th>\n",
       "      <th>1770.</th>\n",
       "      <th>Turning</th>\n",
       "      <th>a</th>\n",
       "      <th>neoclassical</th>\n",
       "      <th>masterpiecewas</th>\n",
       "      <th>Jefferson's</th>\n",
       "      <th>obsession.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thomas  Jefferson  began  building  Monticello   at  theage   of  26.  \\\n",
       "sent0     1.0        1.0    1.0       1.0         1.0  1.0     1.0  1.0  1.0   \n",
       "sent1     0.0        0.0    0.0       0.0         0.0  0.0     0.0  0.0  0.0   \n",
       "sent2     0.0        0.0    0.0       0.0         0.0  0.0     0.0  0.0  0.0   \n",
       "sent3     0.0        0.0    0.0       0.0         1.0  0.0     0.0  0.0  0.0   \n",
       "\n",
       "       Construction  ...  South  Pavilion   in  1770.  Turning    a  \\\n",
       "sent0           0.0  ...    0.0       0.0  0.0    0.0      0.0  0.0   \n",
       "sent1           1.0  ...    0.0       0.0  0.0    0.0      0.0  0.0   \n",
       "sent2           0.0  ...    1.0       1.0  1.0    1.0      0.0  0.0   \n",
       "sent3           0.0  ...    0.0       0.0  0.0    0.0      1.0  1.0   \n",
       "\n",
       "       neoclassical  masterpiecewas  Jefferson's  obsession.  \n",
       "sent0           0.0             0.0          0.0         0.0  \n",
       "sent1           0.0             0.0          0.0         0.0  \n",
       "sent2           0.0             0.0          0.0         0.0  \n",
       "sent3           1.0             1.0          1.0         1.0  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1df1714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91866\\AppData\\Local\\Temp\\ipykernel_19724\\2680053349.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  a = pd.np.array([1,2,3])\n",
      "C:\\Users\\91866\\AppData\\Local\\Temp\\ipykernel_19724\\2680053349.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  b = pd.np.array([[4,5,6],\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24, 30, 36])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.np.array([1,2,3])\n",
    "b = pd.np.array([[4,5,6],\n",
    "                [4,5,6],\n",
    "                [4,5,6]])\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "febd5bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91866\\AppData\\Local\\Temp\\ipykernel_19724\\1537316846.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  a = pd.np.array([[1,2,3],\n",
      "C:\\Users\\91866\\AppData\\Local\\Temp\\ipykernel_19724\\1537316846.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  b = pd.np.array([[4,5,6,4],\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24, 30, 36, 24],\n",
       "       [24, 30, 36, 24]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.np.array([[1,2,3],\n",
    "                [1,2,3]])\n",
    "b = pd.np.array([[4,5,6,4],\n",
    "                [4,5,6,4],\n",
    "                [4,5,6,4]])\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be4c917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b7b3975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sent0.dot(df.sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1417de47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sent0.dot(df.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3223581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sent0.dot(df.sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7eab789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the\\\n",
    "... age of 26.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05abd037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = re.split(r'[-\\s?.,;!]+',sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef19dbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x if x not in '- \\t.;!?' else None,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85a3443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " '...',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5cee487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage\\\n",
    "... Site until 1987.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0bde29dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " '...',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15d2f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '...',\n",
       " 'Awesommmeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casual tokenizer \n",
    "\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello.\\\n",
    "... Awesommmmmmeeeeeeee day :*)\"\"\"\n",
    "from nltk.tokenize import casual_tokenize\n",
    "casual_tokenize(message,reduce_len=True,strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfa083d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson'),\n",
       " ('Jefferson', 'began'),\n",
       " ('began', 'building'),\n",
       " ('building', 'Monticello'),\n",
       " ('Monticello', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', '26')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-grams (2-grams)\n",
    "from nltk.util import ngrams\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the\\\n",
    "... age of 26.\"\"\"\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens = list(filter(lambda x: x if x not in '- \\t\\n.,;!?' else None,tokens))\n",
    "list(ngrams(tokens,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc6bbfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas Jefferson',\n",
       " 'Jefferson began',\n",
       " 'began building',\n",
       " 'building Monticello',\n",
       " 'Monticello at',\n",
       " 'at the',\n",
       " 'the age',\n",
       " 'age of',\n",
       " 'of 26']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the tuples as one from above list\n",
    "tokens = list(ngrams(tokens,2))\n",
    "list(\" \".join(x) for x in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de7cdd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91866\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3af89154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "db8696f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "# Normalizing vocabulary\n",
    "# Casefold\n",
    "tokens = ['House', 'Visitor', 'Center']\n",
    "normalized_tokens = [str.lower(token) for token in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b331eb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dish washer' wash dish\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing vocabulary\n",
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "x = \"dish washer's washed dishes\"\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(word) for word in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "093c0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91866\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Better'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing vocabulary\n",
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('Better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bfff9b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "17ca74c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('goods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4ce373c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodness'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('goodness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a4539e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "631778fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$:': -1.5,\n",
       " '%)': -0.4,\n",
       " '%-)': -1.5,\n",
       " '&-:': -0.4,\n",
       " '&:': -0.7,\n",
       " \"( '}{' )\": 1.6,\n",
       " '(%': -0.9,\n",
       " \"('-:\": 2.2,\n",
       " \"(':\": 2.3,\n",
       " '((-:': 2.1,\n",
       " '(*': 1.1,\n",
       " '(-%': -0.7,\n",
       " '(-*': 1.3,\n",
       " '(-:': 1.6,\n",
       " '(-:0': 2.8,\n",
       " '(-:<': -0.4,\n",
       " '(-:o': 1.5,\n",
       " '(-:O': 1.5,\n",
       " '(-:{': -0.1,\n",
       " '(-:|>*': 1.9,\n",
       " '(-;': 1.3,\n",
       " '(-;|': 2.1,\n",
       " '(8': 2.6,\n",
       " '(:': 2.2,\n",
       " '(:0': 2.4,\n",
       " '(:<': -0.2,\n",
       " '(:o': 2.5,\n",
       " '(:O': 2.5,\n",
       " '(;': 1.1,\n",
       " '(;<': 0.3,\n",
       " '(=': 2.2,\n",
       " '(?:': 2.1,\n",
       " '(^:': 1.5,\n",
       " '(^;': 1.5,\n",
       " '(^;0': 2.0,\n",
       " '(^;o': 1.9,\n",
       " '(o:': 1.6,\n",
       " \")':\": -2.0,\n",
       " \")-':\": -2.1,\n",
       " ')-:': -2.1,\n",
       " ')-:<': -2.2,\n",
       " ')-:{': -2.1,\n",
       " '):': -1.8,\n",
       " '):<': -1.9,\n",
       " '):{': -2.3,\n",
       " ');<': -2.6,\n",
       " '*)': 0.6,\n",
       " '*-)': 0.3,\n",
       " '*-:': 2.1,\n",
       " '*-;': 2.4,\n",
       " '*:': 1.9,\n",
       " '*<|:-)': 1.6,\n",
       " '*\\\\0/*': 2.3,\n",
       " '*^:': 1.6,\n",
       " ',-:': 1.2,\n",
       " \"---'-;-{@\": 2.3,\n",
       " '--<--<@': 2.2,\n",
       " '.-:': -1.2,\n",
       " '..###-:': -1.7,\n",
       " '..###:': -1.9,\n",
       " '/-:': -1.3,\n",
       " '/:': -1.3,\n",
       " '/:<': -1.4,\n",
       " '/=': -0.9,\n",
       " '/^:': -1.0,\n",
       " '/o:': -1.4,\n",
       " '0-8': 0.1,\n",
       " '0-|': -1.2,\n",
       " '0:)': 1.9,\n",
       " '0:-)': 1.4,\n",
       " '0:-3': 1.5,\n",
       " '0:03': 1.9,\n",
       " '0;^)': 1.6,\n",
       " '0_o': -0.3,\n",
       " '10q': 2.1,\n",
       " '1337': 2.1,\n",
       " '143': 3.2,\n",
       " '1432': 2.6,\n",
       " '14aa41': 2.4,\n",
       " '182': -2.9,\n",
       " '187': -3.1,\n",
       " '2g2b4g': 2.8,\n",
       " '2g2bt': -0.1,\n",
       " '2qt': 2.1,\n",
       " '3:(': -2.2,\n",
       " '3:)': 0.5,\n",
       " '3:-(': -2.3,\n",
       " '3:-)': -1.4,\n",
       " '4col': -2.2,\n",
       " '4q': -3.1,\n",
       " '5fs': 1.5,\n",
       " '8)': 1.9,\n",
       " '8-d': 1.7,\n",
       " '8-o': -0.3,\n",
       " '86': -1.6,\n",
       " '8d': 2.9,\n",
       " ':###..': -2.4,\n",
       " ':$': -0.2,\n",
       " ':&': -0.6,\n",
       " \":'(\": -2.2,\n",
       " \":')\": 2.3,\n",
       " \":'-(\": -2.4,\n",
       " \":'-)\": 2.7,\n",
       " ':(': -1.9,\n",
       " ':)': 2.0,\n",
       " ':*': 2.5,\n",
       " ':-###..': -2.5,\n",
       " ':-&': -0.5,\n",
       " ':-(': -1.5,\n",
       " ':-)': 1.3,\n",
       " ':-))': 2.8,\n",
       " ':-*': 1.7,\n",
       " ':-,': 1.1,\n",
       " ':-.': -0.9,\n",
       " ':-/': -1.2,\n",
       " ':-<': -1.5,\n",
       " ':-d': 2.3,\n",
       " ':-D': 2.3,\n",
       " ':-o': 0.1,\n",
       " ':-p': 1.5,\n",
       " ':-[': -1.6,\n",
       " ':-\\\\': -0.9,\n",
       " ':-c': -1.3,\n",
       " ':-|': -0.7,\n",
       " ':-||': -2.5,\n",
       " ':-Þ': 0.9,\n",
       " ':/': -1.4,\n",
       " ':3': 2.3,\n",
       " ':<': -2.1,\n",
       " ':>': 2.1,\n",
       " ':?)': 1.3,\n",
       " ':?c': -1.6,\n",
       " ':@': -2.5,\n",
       " ':d': 2.3,\n",
       " ':D': 2.3,\n",
       " ':l': -1.7,\n",
       " ':o': -0.4,\n",
       " ':p': 1.4,\n",
       " ':s': -1.2,\n",
       " ':[': -2.0,\n",
       " ':\\\\': -1.3,\n",
       " ':]': 2.2,\n",
       " ':^)': 2.1,\n",
       " ':^*': 2.6,\n",
       " ':^/': -1.2,\n",
       " ':^\\\\': -1.0,\n",
       " ':^|': -1.0,\n",
       " ':c': -2.1,\n",
       " ':c)': 2.0,\n",
       " ':o)': 2.1,\n",
       " ':o/': -1.4,\n",
       " ':o\\\\': -1.1,\n",
       " ':o|': -0.6,\n",
       " ':{': -1.9,\n",
       " ':|': -0.4,\n",
       " ':}': 2.1,\n",
       " ':Þ': 1.1,\n",
       " ';)': 0.9,\n",
       " ';-)': 1.0,\n",
       " ';-*': 2.2,\n",
       " ';-]': 0.7,\n",
       " ';d': 0.8,\n",
       " ';D': 0.8,\n",
       " ';]': 0.6,\n",
       " ';^)': 1.4,\n",
       " '</3': -3.0,\n",
       " '<3': 1.9,\n",
       " '<:': 2.1,\n",
       " '<:-|': -1.4,\n",
       " '=)': 2.2,\n",
       " '=-3': 2.0,\n",
       " '=-d': 2.4,\n",
       " '=-D': 2.4,\n",
       " '=/': -1.4,\n",
       " '=3': 2.1,\n",
       " '=d': 2.3,\n",
       " '=D': 2.3,\n",
       " '=l': -1.2,\n",
       " '=\\\\': -1.2,\n",
       " '=]': 1.6,\n",
       " '=p': 1.3,\n",
       " '=|': -0.8,\n",
       " '>-:': -2.0,\n",
       " '>.<': -1.3,\n",
       " '>:': -2.1,\n",
       " '>:(': -2.7,\n",
       " '>:)': 0.4,\n",
       " '>:-(': -2.7,\n",
       " '>:-)': -0.4,\n",
       " '>:/': -1.6,\n",
       " '>:o': -1.2,\n",
       " '>:p': 1.0,\n",
       " '>:[': -2.1,\n",
       " '>:\\\\': -1.7,\n",
       " '>;(': -2.9,\n",
       " '>;)': 0.1,\n",
       " '>_>^': 2.1,\n",
       " '@:': -2.1,\n",
       " '@>-->--': 2.1,\n",
       " \"@}-;-'---\": 2.2,\n",
       " 'aas': 2.5,\n",
       " 'aayf': 2.7,\n",
       " 'afu': -2.9,\n",
       " 'alol': 2.8,\n",
       " 'ambw': 2.9,\n",
       " 'aml': 3.4,\n",
       " 'atab': -1.9,\n",
       " 'awol': -1.3,\n",
       " 'ayc': 0.2,\n",
       " 'ayor': -1.2,\n",
       " 'aug-00': 0.3,\n",
       " 'bfd': -2.7,\n",
       " 'bfe': -2.6,\n",
       " 'bff': 2.9,\n",
       " 'bffn': 1.0,\n",
       " 'bl': 2.3,\n",
       " 'bsod': -2.2,\n",
       " 'btd': -2.1,\n",
       " 'btdt': -0.1,\n",
       " 'bz': 0.4,\n",
       " 'b^d': 2.6,\n",
       " 'cwot': -2.3,\n",
       " \"d-':\": -2.5,\n",
       " 'd8': -3.2,\n",
       " 'd:': 1.2,\n",
       " 'd:<': -3.2,\n",
       " 'd;': -2.9,\n",
       " 'd=': 1.5,\n",
       " 'doa': -2.3,\n",
       " 'dx': -3.0,\n",
       " 'ez': 1.5,\n",
       " 'fav': 2.0,\n",
       " 'fcol': -1.8,\n",
       " 'ff': 1.8,\n",
       " 'ffs': -2.8,\n",
       " 'fkm': -2.4,\n",
       " 'foaf': 1.8,\n",
       " 'ftw': 2.0,\n",
       " 'fu': -3.7,\n",
       " 'fubar': -3.0,\n",
       " 'fwb': 2.5,\n",
       " 'fyi': 0.8,\n",
       " 'fysa': 0.4,\n",
       " 'g1': 1.4,\n",
       " 'gg': 1.2,\n",
       " 'gga': 1.7,\n",
       " 'gigo': -0.6,\n",
       " 'gj': 2.0,\n",
       " 'gl': 1.3,\n",
       " 'gla': 2.5,\n",
       " 'gn': 1.2,\n",
       " 'gr8': 2.7,\n",
       " 'grrr': -0.4,\n",
       " 'gt': 1.1,\n",
       " 'h&k': 2.3,\n",
       " 'hagd': 2.2,\n",
       " 'hagn': 2.2,\n",
       " 'hago': 1.2,\n",
       " 'hak': 1.9,\n",
       " 'hand': 2.2,\n",
       " 'hho1/2k': 1.4,\n",
       " 'hhoj': 2.0,\n",
       " 'hhok': 0.9,\n",
       " 'hugz': 2.0,\n",
       " 'hi5': 1.9,\n",
       " 'idk': -0.4,\n",
       " 'ijs': 0.7,\n",
       " 'ilu': 3.4,\n",
       " 'iluaaf': 2.7,\n",
       " 'ily': 3.4,\n",
       " 'ily2': 2.6,\n",
       " 'iou': 0.7,\n",
       " 'iyq': 2.3,\n",
       " 'j/j': 2.0,\n",
       " 'j/k': 1.6,\n",
       " 'j/p': 1.4,\n",
       " 'j/t': -0.2,\n",
       " 'j/w': 1.0,\n",
       " 'j4f': 1.4,\n",
       " 'j4g': 1.7,\n",
       " 'jho': 0.8,\n",
       " 'jhomf': 1.0,\n",
       " 'jj': 1.0,\n",
       " 'jk': 0.9,\n",
       " 'jp': 0.8,\n",
       " 'jt': 0.9,\n",
       " 'jw': 1.6,\n",
       " 'jealz': -1.2,\n",
       " 'k4y': 2.3,\n",
       " 'kfy': 2.3,\n",
       " 'kia': -3.2,\n",
       " 'kk': 1.5,\n",
       " 'kmuf': 2.2,\n",
       " 'l': 2.0,\n",
       " 'l&r': 2.2,\n",
       " 'laoj': 1.3,\n",
       " 'lmao': 2.9,\n",
       " 'lmbao': 1.8,\n",
       " 'lmfao': 2.5,\n",
       " 'lmso': 2.7,\n",
       " 'lol': 1.8,\n",
       " 'lolz': 2.7,\n",
       " 'lts': 1.6,\n",
       " 'ly': 2.6,\n",
       " 'ly4e': 2.7,\n",
       " 'lya': 3.3,\n",
       " 'lyb': 3.0,\n",
       " 'lyl': 3.1,\n",
       " 'lylab': 2.7,\n",
       " 'lylas': 2.6,\n",
       " 'lylb': 1.6,\n",
       " 'm8': 1.4,\n",
       " 'mia': -1.2,\n",
       " 'mml': 2.0,\n",
       " 'mofo': -2.4,\n",
       " 'muah': 2.3,\n",
       " 'mubar': -1.0,\n",
       " 'musm': 0.9,\n",
       " 'mwah': 2.5,\n",
       " 'n1': 1.9,\n",
       " 'nbd': 1.3,\n",
       " 'nbif': -0.5,\n",
       " 'nfc': -2.7,\n",
       " 'nfw': -2.4,\n",
       " 'nh': 2.2,\n",
       " 'nimby': -0.8,\n",
       " 'nimjd': -0.7,\n",
       " 'nimq': -0.2,\n",
       " 'nimy': -1.4,\n",
       " 'nitl': -1.5,\n",
       " 'nme': -2.1,\n",
       " 'noyb': -0.7,\n",
       " 'np': 1.4,\n",
       " 'ntmu': 1.4,\n",
       " 'o-8': -0.5,\n",
       " 'o-:': -0.3,\n",
       " 'o-|': -1.1,\n",
       " 'o.o': -0.8,\n",
       " 'O.o': -0.6,\n",
       " 'o.O': -0.6,\n",
       " 'o:': -0.2,\n",
       " 'o:)': 1.5,\n",
       " 'o:-)': 2.0,\n",
       " 'o:-3': 2.2,\n",
       " 'o:3': 2.3,\n",
       " 'o:<': -0.3,\n",
       " 'o;^)': 1.6,\n",
       " 'ok': 1.2,\n",
       " 'o_o': -0.5,\n",
       " 'O_o': -0.5,\n",
       " 'o_O': -0.5,\n",
       " 'pita': -2.4,\n",
       " 'pls': 0.3,\n",
       " 'plz': 0.3,\n",
       " 'pmbi': 0.8,\n",
       " 'pmfji': 0.3,\n",
       " 'pmji': 0.7,\n",
       " 'po': -2.6,\n",
       " 'ptl': 2.6,\n",
       " 'pu': -1.1,\n",
       " 'qq': -2.2,\n",
       " 'qt': 1.8,\n",
       " 'r&r': 2.4,\n",
       " 'rofl': 2.7,\n",
       " 'roflmao': 2.5,\n",
       " 'rotfl': 2.6,\n",
       " 'rotflmao': 2.8,\n",
       " 'rotflmfao': 2.5,\n",
       " 'rotflol': 3.0,\n",
       " 'rotgl': 2.9,\n",
       " 'rotglmao': 1.8,\n",
       " 's:': -1.1,\n",
       " 'sapfu': -1.1,\n",
       " 'sete': 2.8,\n",
       " 'sfete': 2.7,\n",
       " 'sgtm': 2.4,\n",
       " 'slap': 0.6,\n",
       " 'slaw': 2.1,\n",
       " 'smh': -1.3,\n",
       " 'snafu': -2.5,\n",
       " 'sob': -1.0,\n",
       " 'swak': 2.3,\n",
       " 'tgif': 2.3,\n",
       " 'thks': 1.4,\n",
       " 'thx': 1.5,\n",
       " 'tia': 2.3,\n",
       " 'tmi': -0.3,\n",
       " 'tnx': 1.1,\n",
       " 'true': 1.8,\n",
       " 'tx': 1.5,\n",
       " 'txs': 1.1,\n",
       " 'ty': 1.6,\n",
       " 'tyvm': 2.5,\n",
       " 'urw': 1.9,\n",
       " 'vbg': 2.1,\n",
       " 'vbs': 3.1,\n",
       " 'vip': 2.3,\n",
       " 'vwd': 2.6,\n",
       " 'vwp': 2.1,\n",
       " 'wag': -0.2,\n",
       " 'wd': 2.7,\n",
       " 'wilco': 0.9,\n",
       " 'wp': 1.0,\n",
       " 'wtf': -2.8,\n",
       " 'wtg': 2.1,\n",
       " 'wth': -2.4,\n",
       " 'x-d': 2.6,\n",
       " 'x-p': 1.7,\n",
       " 'xd': 2.8,\n",
       " 'xlnt': 3.0,\n",
       " 'xoxo': 3.0,\n",
       " 'xoxozzz': 2.3,\n",
       " 'xp': 1.6,\n",
       " 'xqzt': 1.6,\n",
       " 'xtc': 0.8,\n",
       " 'yolo': 1.1,\n",
       " 'yoyo': 0.4,\n",
       " 'yvw': 1.6,\n",
       " 'yw': 1.8,\n",
       " 'ywia': 2.5,\n",
       " 'zzz': -1.2,\n",
       " '[-;': 0.5,\n",
       " '[:': 1.3,\n",
       " '[;': 1.0,\n",
       " '[=': 1.7,\n",
       " '\\\\-:': -1.0,\n",
       " '\\\\:': -1.0,\n",
       " '\\\\:<': -1.7,\n",
       " '\\\\=': -1.1,\n",
       " '\\\\^:': -1.3,\n",
       " '\\\\o/': 2.2,\n",
       " '\\\\o:': -1.2,\n",
       " ']-:': -2.1,\n",
       " ']:': -1.6,\n",
       " ']:<': -2.5,\n",
       " '^<_<': 1.4,\n",
       " '^urs': -2.8,\n",
       " 'abandon': -1.9,\n",
       " 'abandoned': -2.0,\n",
       " 'abandoner': -1.9,\n",
       " 'abandoners': -1.9,\n",
       " 'abandoning': -1.6,\n",
       " 'abandonment': -2.4,\n",
       " 'abandonments': -1.7,\n",
       " 'abandons': -1.3,\n",
       " 'abducted': -2.3,\n",
       " 'abduction': -2.8,\n",
       " 'abductions': -2.0,\n",
       " 'abhor': -2.0,\n",
       " 'abhorred': -2.4,\n",
       " 'abhorrent': -3.1,\n",
       " 'abhors': -2.9,\n",
       " 'abilities': 1.0,\n",
       " 'ability': 1.3,\n",
       " 'aboard': 0.1,\n",
       " 'absentee': -1.1,\n",
       " 'absentees': -0.8,\n",
       " 'absolve': 1.2,\n",
       " 'absolved': 1.5,\n",
       " 'absolves': 1.3,\n",
       " 'absolving': 1.6,\n",
       " 'abuse': -3.2,\n",
       " 'abused': -2.3,\n",
       " 'abuser': -2.6,\n",
       " 'abusers': -2.6,\n",
       " 'abuses': -2.6,\n",
       " 'abusing': -2.0,\n",
       " 'abusive': -3.2,\n",
       " 'abusively': -2.8,\n",
       " 'abusiveness': -2.5,\n",
       " 'abusivenesses': -3.0,\n",
       " 'accept': 1.6,\n",
       " 'acceptabilities': 1.6,\n",
       " 'acceptability': 1.1,\n",
       " 'acceptable': 1.3,\n",
       " 'acceptableness': 1.3,\n",
       " 'acceptably': 1.5,\n",
       " 'acceptance': 2.0,\n",
       " 'acceptances': 1.7,\n",
       " 'acceptant': 1.6,\n",
       " 'acceptation': 1.3,\n",
       " 'acceptations': 0.9,\n",
       " 'accepted': 1.1,\n",
       " 'accepting': 1.6,\n",
       " 'accepts': 1.3,\n",
       " 'accident': -2.1,\n",
       " 'accidental': -0.3,\n",
       " 'accidentally': -1.4,\n",
       " 'accidents': -1.3,\n",
       " 'accomplish': 1.8,\n",
       " 'accomplished': 1.9,\n",
       " 'accomplishes': 1.7,\n",
       " 'accusation': -1.0,\n",
       " 'accusations': -1.3,\n",
       " 'accuse': -0.8,\n",
       " 'accused': -1.2,\n",
       " 'accuses': -1.4,\n",
       " 'accusing': -0.7,\n",
       " 'ache': -1.6,\n",
       " 'ached': -1.6,\n",
       " 'aches': -1.0,\n",
       " 'achievable': 1.3,\n",
       " 'aching': -2.2,\n",
       " 'acquit': 0.8,\n",
       " 'acquits': 0.1,\n",
       " 'acquitted': 1.0,\n",
       " 'acquitting': 1.3,\n",
       " 'acrimonious': -1.7,\n",
       " 'active': 1.7,\n",
       " 'actively': 1.3,\n",
       " 'activeness': 0.6,\n",
       " 'activenesses': 0.8,\n",
       " 'actives': 1.1,\n",
       " 'adequate': 0.9,\n",
       " 'admirability': 2.4,\n",
       " 'admirable': 2.6,\n",
       " 'admirableness': 2.2,\n",
       " 'admirably': 2.5,\n",
       " 'admiral': 1.3,\n",
       " 'admirals': 1.5,\n",
       " 'admiralties': 1.6,\n",
       " 'admiralty': 1.2,\n",
       " 'admiration': 2.5,\n",
       " 'admirations': 1.6,\n",
       " 'admire': 2.1,\n",
       " 'admired': 2.3,\n",
       " 'admirer': 1.8,\n",
       " 'admirers': 1.7,\n",
       " 'admires': 1.5,\n",
       " 'admiring': 1.6,\n",
       " 'admiringly': 2.3,\n",
       " 'admit': 0.8,\n",
       " 'admits': 1.2,\n",
       " 'admitted': 0.4,\n",
       " 'admonished': -1.9,\n",
       " 'adopt': 0.7,\n",
       " 'adopts': 0.7,\n",
       " 'adorability': 2.2,\n",
       " 'adorable': 2.2,\n",
       " 'adorableness': 2.5,\n",
       " 'adorably': 2.1,\n",
       " 'adoration': 2.9,\n",
       " 'adorations': 2.2,\n",
       " 'adore': 2.6,\n",
       " 'adored': 1.8,\n",
       " 'adorer': 1.7,\n",
       " 'adorers': 2.1,\n",
       " 'adores': 1.6,\n",
       " 'adoring': 2.6,\n",
       " 'adoringly': 2.4,\n",
       " 'adorn': 0.9,\n",
       " 'adorned': 0.8,\n",
       " 'adorner': 1.3,\n",
       " 'adorners': 0.9,\n",
       " 'adorning': 1.0,\n",
       " 'adornment': 1.3,\n",
       " 'adornments': 0.8,\n",
       " 'adorns': 0.5,\n",
       " 'advanced': 1.0,\n",
       " 'advantage': 1.0,\n",
       " 'advantaged': 1.4,\n",
       " 'advantageous': 1.5,\n",
       " 'advantageously': 1.9,\n",
       " 'advantageousness': 1.6,\n",
       " 'advantages': 1.5,\n",
       " 'advantaging': 1.6,\n",
       " 'adventure': 1.3,\n",
       " 'adventured': 1.3,\n",
       " 'adventurer': 1.2,\n",
       " 'adventurers': 0.9,\n",
       " 'adventures': 1.4,\n",
       " 'adventuresome': 1.7,\n",
       " 'adventuresomeness': 1.3,\n",
       " 'adventuress': 0.8,\n",
       " 'adventuresses': 1.4,\n",
       " 'adventuring': 2.3,\n",
       " 'adventurism': 1.5,\n",
       " 'adventurist': 1.4,\n",
       " 'adventuristic': 1.7,\n",
       " 'adventurists': 1.2,\n",
       " 'adventurous': 1.4,\n",
       " 'adventurously': 1.3,\n",
       " 'adventurousness': 1.8,\n",
       " 'adversarial': -1.5,\n",
       " 'adversaries': -1.0,\n",
       " 'adversary': -0.8,\n",
       " 'adversative': -1.2,\n",
       " 'adversatively': -0.1,\n",
       " 'adversatives': -1.0,\n",
       " 'adverse': -1.5,\n",
       " 'adversely': -0.8,\n",
       " 'adverseness': -0.6,\n",
       " 'adversities': -1.5,\n",
       " 'adversity': -1.8,\n",
       " 'affected': -0.6,\n",
       " 'affection': 2.4,\n",
       " 'affectional': 1.9,\n",
       " 'affectionally': 1.5,\n",
       " 'affectionate': 1.9,\n",
       " 'affectionately': 2.2,\n",
       " 'affectioned': 1.8,\n",
       " 'affectionless': -2.0,\n",
       " 'affections': 1.5,\n",
       " 'afflicted': -1.5,\n",
       " 'affronted': 0.2,\n",
       " 'aggravate': -2.5,\n",
       " 'aggravated': -1.9,\n",
       " 'aggravates': -1.9,\n",
       " 'aggravating': -1.2,\n",
       " 'aggress': -1.3,\n",
       " 'aggressed': -1.4,\n",
       " 'aggresses': -0.5,\n",
       " 'aggressing': -0.6,\n",
       " 'aggression': -1.2,\n",
       " 'aggressions': -1.3,\n",
       " 'aggressive': -0.6,\n",
       " 'aggressively': -1.3,\n",
       " 'aggressiveness': -1.8,\n",
       " 'aggressivities': -1.4,\n",
       " 'aggressivity': -0.6,\n",
       " 'aggressor': -0.8,\n",
       " 'aggressors': -0.9,\n",
       " 'aghast': -1.9,\n",
       " 'agitate': -1.7,\n",
       " 'agitated': -2.0,\n",
       " 'agitatedly': -1.6,\n",
       " 'agitates': -1.4,\n",
       " 'agitating': -1.8,\n",
       " 'agitation': -1.0,\n",
       " 'agitational': -1.2,\n",
       " 'agitations': -1.3,\n",
       " 'agitative': -1.3,\n",
       " 'agitato': -0.1,\n",
       " 'agitator': -1.4,\n",
       " 'agitators': -2.1,\n",
       " 'agog': 1.9,\n",
       " 'agonise': -2.1,\n",
       " 'agonised': -2.3,\n",
       " 'agonises': -2.4,\n",
       " 'agonising': -1.5,\n",
       " 'agonize': -2.3,\n",
       " 'agonized': -2.2,\n",
       " 'agonizes': -2.3,\n",
       " 'agonizing': -2.7,\n",
       " 'agonizingly': -2.3,\n",
       " 'agony': -1.8,\n",
       " 'agree': 1.5,\n",
       " 'agreeability': 1.9,\n",
       " 'agreeable': 1.8,\n",
       " 'agreeableness': 1.8,\n",
       " 'agreeablenesses': 1.3,\n",
       " 'agreeably': 1.6,\n",
       " 'agreed': 1.1,\n",
       " 'agreeing': 1.4,\n",
       " 'agreement': 2.2,\n",
       " 'agreements': 1.1,\n",
       " 'agrees': 0.8,\n",
       " 'alarm': -1.4,\n",
       " 'alarmed': -1.4,\n",
       " 'alarming': -0.5,\n",
       " 'alarmingly': -2.6,\n",
       " 'alarmism': -0.3,\n",
       " 'alarmists': -1.1,\n",
       " 'alarms': -1.1,\n",
       " 'alas': -1.1,\n",
       " 'alert': 1.2,\n",
       " 'alienation': -1.1,\n",
       " 'alive': 1.6,\n",
       " 'allergic': -1.2,\n",
       " 'allow': 0.9,\n",
       " 'alone': -1.0,\n",
       " 'alright': 1.0,\n",
       " 'amaze': 2.5,\n",
       " 'amazed': 2.2,\n",
       " 'amazedly': 2.1,\n",
       " 'amazement': 2.5,\n",
       " 'amazements': 2.2,\n",
       " 'amazes': 2.2,\n",
       " 'amazing': 2.8,\n",
       " 'amazon': 0.7,\n",
       " 'amazonite': 0.2,\n",
       " 'amazons': -0.1,\n",
       " 'amazonstone': 1.0,\n",
       " 'amazonstones': 0.2,\n",
       " 'ambitious': 2.1,\n",
       " 'ambivalent': 0.5,\n",
       " 'amor': 3.0,\n",
       " 'amoral': -1.6,\n",
       " 'amoralism': -0.7,\n",
       " 'amoralisms': -0.7,\n",
       " 'amoralities': -1.2,\n",
       " 'amorality': -1.5,\n",
       " 'amorally': -1.0,\n",
       " 'amoretti': 0.2,\n",
       " 'amoretto': 0.6,\n",
       " 'amorettos': 0.3,\n",
       " 'amorino': 1.2,\n",
       " 'amorist': 1.6,\n",
       " 'amoristic': 1.0,\n",
       " 'amorists': 0.1,\n",
       " 'amoroso': 2.3,\n",
       " 'amorous': 1.8,\n",
       " 'amorously': 2.3,\n",
       " 'amorousness': 2.0,\n",
       " 'amorphous': -0.2,\n",
       " 'amorphously': 0.1,\n",
       " 'amorphousness': 0.3,\n",
       " 'amort': -2.1,\n",
       " 'amortise': 0.5,\n",
       " 'amortised': -0.2,\n",
       " 'amortises': 0.1,\n",
       " 'amortizable': 0.5,\n",
       " 'amortization': 0.6,\n",
       " 'amortizations': 0.2,\n",
       " 'amortize': -0.1,\n",
       " 'amortized': 0.8,\n",
       " 'amortizes': 0.6,\n",
       " 'amortizing': 0.8,\n",
       " 'amusable': 0.7,\n",
       " 'amuse': 1.7,\n",
       " 'amused': 1.8,\n",
       " 'amusedly': 2.2,\n",
       " 'amusement': 1.5,\n",
       " 'amusements': 1.5,\n",
       " 'amuser': 1.1,\n",
       " 'amusers': 1.3,\n",
       " 'amuses': 1.7,\n",
       " 'amusia': 0.3,\n",
       " 'amusias': -0.4,\n",
       " 'amusing': 1.6,\n",
       " 'amusingly': 0.8,\n",
       " 'amusingness': 1.8,\n",
       " 'amusive': 1.7,\n",
       " 'anger': -2.7,\n",
       " 'angered': -2.3,\n",
       " 'angering': -2.2,\n",
       " 'angerly': -1.9,\n",
       " 'angers': -2.3,\n",
       " 'angrier': -2.3,\n",
       " 'angriest': -3.1,\n",
       " 'angrily': -1.8,\n",
       " 'angriness': -1.7,\n",
       " 'angry': -2.3,\n",
       " 'anguish': -2.9,\n",
       " 'anguished': -1.8,\n",
       " 'anguishes': -2.1,\n",
       " 'anguishing': -2.7,\n",
       " 'animosity': -1.9,\n",
       " 'annoy': -1.9,\n",
       " 'annoyance': -1.3,\n",
       " 'annoyances': -1.8,\n",
       " 'annoyed': -1.6,\n",
       " 'annoyer': -2.2,\n",
       " 'annoyers': -1.5,\n",
       " 'annoying': -1.7,\n",
       " 'annoys': -1.8,\n",
       " 'antagonism': -1.9,\n",
       " 'antagonisms': -1.2,\n",
       " 'antagonist': -1.9,\n",
       " 'antagonistic': -1.7,\n",
       " 'antagonistically': -2.2,\n",
       " 'antagonists': -1.7,\n",
       " 'antagonize': -2.0,\n",
       " 'antagonized': -1.4,\n",
       " 'antagonizes': -0.5,\n",
       " 'antagonizing': -2.7,\n",
       " 'anti': -1.3,\n",
       " 'anticipation': 0.4,\n",
       " 'anxieties': -0.6,\n",
       " 'anxiety': -0.7,\n",
       " 'anxious': -1.0,\n",
       " 'anxiously': -0.9,\n",
       " 'anxiousness': -1.0,\n",
       " 'aok': 2.0,\n",
       " 'apathetic': -1.2,\n",
       " 'apathetically': -0.4,\n",
       " 'apathies': -0.6,\n",
       " 'apathy': -1.2,\n",
       " 'apeshit': -0.9,\n",
       " 'apocalyptic': -3.4,\n",
       " 'apologise': 1.6,\n",
       " 'apologised': 0.4,\n",
       " 'apologises': 0.8,\n",
       " 'apologising': 0.2,\n",
       " 'apologize': 0.4,\n",
       " 'apologized': 1.3,\n",
       " 'apologizes': 1.5,\n",
       " 'apologizing': -0.3,\n",
       " 'apology': 0.2,\n",
       " 'appall': -2.4,\n",
       " 'appalled': -2.0,\n",
       " 'appalling': -1.5,\n",
       " 'appallingly': -2.0,\n",
       " 'appalls': -1.9,\n",
       " 'appease': 1.1,\n",
       " 'appeased': 0.9,\n",
       " 'appeases': 0.9,\n",
       " 'appeasing': 1.0,\n",
       " 'applaud': 2.0,\n",
       " 'applauded': 1.5,\n",
       " 'applauding': 2.1,\n",
       " 'applauds': 1.4,\n",
       " 'applause': 1.8,\n",
       " 'appreciate': 1.7,\n",
       " 'appreciated': 2.3,\n",
       " 'appreciates': 2.3,\n",
       " 'appreciating': 1.9,\n",
       " 'appreciation': 2.3,\n",
       " 'appreciations': 1.7,\n",
       " 'appreciative': 2.6,\n",
       " 'appreciatively': 1.8,\n",
       " 'appreciativeness': 1.6,\n",
       " 'appreciator': 2.6,\n",
       " 'appreciators': 1.5,\n",
       " 'appreciatory': 1.7,\n",
       " 'apprehensible': 1.1,\n",
       " 'apprehensibly': -0.2,\n",
       " 'apprehension': -2.1,\n",
       " 'apprehensions': -0.9,\n",
       " 'apprehensively': -0.3,\n",
       " 'apprehensiveness': -0.7,\n",
       " 'approval': 2.1,\n",
       " 'approved': 1.8,\n",
       " 'approves': 1.7,\n",
       " 'ardent': 2.1,\n",
       " 'arguable': -1.0,\n",
       " 'arguably': -1.0,\n",
       " 'argue': -1.4,\n",
       " 'argued': -1.5,\n",
       " 'arguer': -1.6,\n",
       " 'arguers': -1.4,\n",
       " 'argues': -1.6,\n",
       " 'arguing': -2.0,\n",
       " 'argument': -1.5,\n",
       " 'argumentative': -1.5,\n",
       " 'argumentatively': -1.8,\n",
       " 'argumentive': -1.5,\n",
       " 'arguments': -1.7,\n",
       " 'arrest': -1.4,\n",
       " 'arrested': -2.1,\n",
       " 'arrests': -1.9,\n",
       " 'arrogance': -2.4,\n",
       " 'arrogances': -1.9,\n",
       " 'arrogant': -2.2,\n",
       " 'arrogantly': -1.8,\n",
       " 'ashamed': -2.1,\n",
       " 'ashamedly': -1.7,\n",
       " 'ass': -2.5,\n",
       " 'assassination': -2.9,\n",
       " 'assassinations': -2.7,\n",
       " 'assault': -2.8,\n",
       " 'assaulted': -2.4,\n",
       " 'assaulting': -2.3,\n",
       " 'assaultive': -2.8,\n",
       " 'assaults': -2.5,\n",
       " 'asset': 1.5,\n",
       " 'assets': 0.7,\n",
       " 'assfucking': -2.5,\n",
       " 'assholes': -2.8,\n",
       " 'assurance': 1.4,\n",
       " 'assurances': 1.4,\n",
       " 'assure': 1.4,\n",
       " 'assured': 1.5,\n",
       " 'assuredly': 1.6,\n",
       " 'assuredness': 1.4,\n",
       " 'assurer': 0.9,\n",
       " 'assurers': 1.1,\n",
       " 'assures': 1.3,\n",
       " 'assurgent': 1.3,\n",
       " 'assuring': 1.6,\n",
       " 'assuror': 0.5,\n",
       " 'assurors': 0.7,\n",
       " 'astonished': 1.6,\n",
       " 'astound': 1.7,\n",
       " 'astounded': 1.8,\n",
       " 'astounding': 1.8,\n",
       " 'astoundingly': 2.1,\n",
       " 'astounds': 2.1,\n",
       " 'attachment': 1.2,\n",
       " 'attachments': 1.1,\n",
       " 'attack': -2.1,\n",
       " 'attacked': -2.0,\n",
       " 'attacker': -2.7,\n",
       " 'attackers': -2.7,\n",
       " 'attacking': -2.0,\n",
       " 'attacks': -1.9,\n",
       " 'attract': 1.5,\n",
       " 'attractancy': 0.9,\n",
       " 'attractant': 1.3,\n",
       " 'attractants': 1.4,\n",
       " 'attracted': 1.8,\n",
       " 'attracting': 2.1,\n",
       " 'attraction': 2.0,\n",
       " 'attractions': 1.8,\n",
       " 'attractive': 1.9,\n",
       " 'attractively': 2.2,\n",
       " 'attractiveness': 1.8,\n",
       " 'attractivenesses': 2.1,\n",
       " 'attractor': 1.2,\n",
       " 'attractors': 1.2,\n",
       " 'attracts': 1.7,\n",
       " 'audacious': 0.9,\n",
       " 'authority': 0.3,\n",
       " 'aversion': -1.9,\n",
       " 'aversions': -1.1,\n",
       " 'aversive': -1.6,\n",
       " 'aversively': -0.8,\n",
       " 'avert': -0.7,\n",
       " 'averted': -0.3,\n",
       " 'averts': -0.4,\n",
       " 'avid': 1.2,\n",
       " 'avoid': -1.2,\n",
       " 'avoidance': -1.7,\n",
       " 'avoidances': -1.1,\n",
       " 'avoided': -1.4,\n",
       " 'avoider': -1.8,\n",
       " 'avoiders': -1.4,\n",
       " 'avoiding': -1.4,\n",
       " 'avoids': -0.7,\n",
       " 'await': 0.4,\n",
       " 'awaited': -0.1,\n",
       " 'awaits': 0.3,\n",
       " 'award': 2.5,\n",
       " 'awardable': 2.4,\n",
       " 'awarded': 1.7,\n",
       " 'awardee': 1.8,\n",
       " 'awardees': 1.2,\n",
       " 'awarder': 0.9,\n",
       " 'awarders': 1.3,\n",
       " 'awarding': 1.9,\n",
       " 'awards': 2.0,\n",
       " 'awesome': 3.1,\n",
       " 'awful': -2.0,\n",
       " 'awkward': -0.6,\n",
       " 'awkwardly': -1.3,\n",
       " 'awkwardness': -0.7,\n",
       " 'axe': -0.4,\n",
       " 'axed': -1.3,\n",
       " 'backed': 0.1,\n",
       " 'backing': 0.1,\n",
       " 'backs': -0.2,\n",
       " 'bad': -2.5,\n",
       " 'badass': -0.6,\n",
       " 'badly': -2.1,\n",
       " 'bailout': -0.4,\n",
       " 'bamboozle': -1.5,\n",
       " 'bamboozled': -1.5,\n",
       " 'bamboozles': -1.5,\n",
       " 'ban': -2.6,\n",
       " 'banish': -1.9,\n",
       " 'bankrupt': -2.6,\n",
       " 'bankster': -2.1,\n",
       " 'banned': -2.0,\n",
       " 'bargain': 0.8,\n",
       " 'barrier': -0.5,\n",
       " 'bashful': -0.1,\n",
       " 'bashfully': 0.2,\n",
       " 'bashfulness': -0.8,\n",
       " 'bastard': -2.5,\n",
       " 'bastardies': -1.8,\n",
       " 'bastardise': -2.1,\n",
       " 'bastardised': -2.3,\n",
       " 'bastardises': -2.3,\n",
       " 'bastardising': -2.6,\n",
       " 'bastardization': -2.4,\n",
       " 'bastardizations': -2.1,\n",
       " 'bastardize': -2.4,\n",
       " 'bastardized': -2.0,\n",
       " 'bastardizes': -1.8,\n",
       " 'bastardizing': -2.3,\n",
       " 'bastardly': -2.7,\n",
       " 'bastards': -3.0,\n",
       " 'bastardy': -2.7,\n",
       " 'battle': -1.6,\n",
       " 'battled': -1.2,\n",
       " 'battlefield': -1.6,\n",
       " 'battlefields': -0.9,\n",
       " 'battlefront': -1.2,\n",
       " 'battlefronts': -0.8,\n",
       " 'battleground': -1.7,\n",
       " 'battlegrounds': -0.6,\n",
       " 'battlement': -0.4,\n",
       " 'battlements': -0.4,\n",
       " 'battler': -0.8,\n",
       " 'battlers': -0.2,\n",
       " 'battles': -1.6,\n",
       " 'battleship': -0.1,\n",
       " 'battleships': -0.5,\n",
       " 'battlewagon': -0.3,\n",
       " 'battlewagons': -0.5,\n",
       " 'battling': -1.1,\n",
       " 'beaten': -1.8,\n",
       " 'beatific': 1.8,\n",
       " 'beating': -2.0,\n",
       " 'beaut': 1.6,\n",
       " 'beauteous': 2.5,\n",
       " 'beauteously': 2.6,\n",
       " 'beauteousness': 2.7,\n",
       " 'beautician': 1.2,\n",
       " 'beauticians': 0.4,\n",
       " ...}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis with VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "sa.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "85c4e67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$:', -1.5),\n",
       " ('%)', -0.4),\n",
       " ('%-)', -1.5),\n",
       " ('&-:', -0.4),\n",
       " ('&:', -0.7),\n",
       " (\"( '}{' )\", 1.6),\n",
       " ('(%', -0.9),\n",
       " (\"('-:\", 2.2),\n",
       " (\"(':\", 2.3),\n",
       " ('((-:', 2.1),\n",
       " ('(*', 1.1),\n",
       " ('(-%', -0.7),\n",
       " ('(-*', 1.3),\n",
       " ('(-:', 1.6),\n",
       " ('(-:0', 2.8),\n",
       " ('(-:<', -0.4),\n",
       " ('(-:o', 1.5),\n",
       " ('(-:O', 1.5),\n",
       " ('(-:{', -0.1),\n",
       " ('(-:|>*', 1.9),\n",
       " ('(-;', 1.3),\n",
       " ('(-;|', 2.1),\n",
       " ('(8', 2.6),\n",
       " ('(:', 2.2),\n",
       " ('(:0', 2.4),\n",
       " ('(:<', -0.2),\n",
       " ('(:o', 2.5),\n",
       " ('(:O', 2.5),\n",
       " ('(;', 1.1),\n",
       " ('(;<', 0.3),\n",
       " ('(=', 2.2),\n",
       " ('(?:', 2.1),\n",
       " ('(^:', 1.5),\n",
       " ('(^;', 1.5),\n",
       " ('(^;0', 2.0),\n",
       " ('(^;o', 1.9),\n",
       " ('(o:', 1.6),\n",
       " (\")':\", -2.0),\n",
       " (\")-':\", -2.1),\n",
       " (')-:', -2.1),\n",
       " (')-:<', -2.2),\n",
       " (')-:{', -2.1),\n",
       " ('):', -1.8),\n",
       " ('):<', -1.9),\n",
       " ('):{', -2.3),\n",
       " (');<', -2.6),\n",
       " ('*)', 0.6),\n",
       " ('*-)', 0.3),\n",
       " ('*-:', 2.1),\n",
       " ('*-;', 2.4),\n",
       " ('*:', 1.9),\n",
       " ('*<|:-)', 1.6),\n",
       " ('*\\\\0/*', 2.3),\n",
       " ('*^:', 1.6),\n",
       " (',-:', 1.2),\n",
       " (\"---'-;-{@\", 2.3),\n",
       " ('--<--<@', 2.2),\n",
       " ('.-:', -1.2),\n",
       " ('..###-:', -1.7),\n",
       " ('..###:', -1.9),\n",
       " ('/-:', -1.3),\n",
       " ('/:', -1.3),\n",
       " ('/:<', -1.4),\n",
       " ('/=', -0.9),\n",
       " ('/^:', -1.0),\n",
       " ('/o:', -1.4),\n",
       " ('0-8', 0.1),\n",
       " ('0-|', -1.2),\n",
       " ('0:)', 1.9),\n",
       " ('0:-)', 1.4),\n",
       " ('0:-3', 1.5),\n",
       " ('0:03', 1.9),\n",
       " ('0;^)', 1.6),\n",
       " ('0_o', -0.3),\n",
       " ('10q', 2.1),\n",
       " ('1337', 2.1),\n",
       " ('143', 3.2),\n",
       " ('1432', 2.6),\n",
       " ('14aa41', 2.4),\n",
       " ('182', -2.9),\n",
       " ('187', -3.1),\n",
       " ('2g2b4g', 2.8),\n",
       " ('2g2bt', -0.1),\n",
       " ('2qt', 2.1),\n",
       " ('3:(', -2.2),\n",
       " ('3:)', 0.5),\n",
       " ('3:-(', -2.3),\n",
       " ('3:-)', -1.4),\n",
       " ('4col', -2.2),\n",
       " ('4q', -3.1),\n",
       " ('5fs', 1.5),\n",
       " ('8)', 1.9),\n",
       " ('8-d', 1.7),\n",
       " ('8-o', -0.3),\n",
       " ('86', -1.6),\n",
       " ('8d', 2.9),\n",
       " (':###..', -2.4),\n",
       " (':$', -0.2),\n",
       " (':&', -0.6),\n",
       " (\":'(\", -2.2),\n",
       " (\":')\", 2.3),\n",
       " (\":'-(\", -2.4),\n",
       " (\":'-)\", 2.7),\n",
       " (':(', -1.9),\n",
       " (':)', 2.0),\n",
       " (':*', 2.5),\n",
       " (':-###..', -2.5),\n",
       " (':-&', -0.5),\n",
       " (':-(', -1.5),\n",
       " (':-)', 1.3),\n",
       " (':-))', 2.8),\n",
       " (':-*', 1.7),\n",
       " (':-,', 1.1),\n",
       " (':-.', -0.9),\n",
       " (':-/', -1.2),\n",
       " (':-<', -1.5),\n",
       " (':-d', 2.3),\n",
       " (':-D', 2.3),\n",
       " (':-o', 0.1),\n",
       " (':-p', 1.5),\n",
       " (':-[', -1.6),\n",
       " (':-\\\\', -0.9),\n",
       " (':-c', -1.3),\n",
       " (':-|', -0.7),\n",
       " (':-||', -2.5),\n",
       " (':-Þ', 0.9),\n",
       " (':/', -1.4),\n",
       " (':3', 2.3),\n",
       " (':<', -2.1),\n",
       " (':>', 2.1),\n",
       " (':?)', 1.3),\n",
       " (':?c', -1.6),\n",
       " (':@', -2.5),\n",
       " (':d', 2.3),\n",
       " (':D', 2.3),\n",
       " (':l', -1.7),\n",
       " (':o', -0.4),\n",
       " (':p', 1.4),\n",
       " (':s', -1.2),\n",
       " (':[', -2.0),\n",
       " (':\\\\', -1.3),\n",
       " (':]', 2.2),\n",
       " (':^)', 2.1),\n",
       " (':^*', 2.6),\n",
       " (':^/', -1.2),\n",
       " (':^\\\\', -1.0),\n",
       " (':^|', -1.0),\n",
       " (':c', -2.1),\n",
       " (':c)', 2.0),\n",
       " (':o)', 2.1),\n",
       " (':o/', -1.4),\n",
       " (':o\\\\', -1.1),\n",
       " (':o|', -0.6),\n",
       " (':{', -1.9),\n",
       " (':|', -0.4),\n",
       " (':}', 2.1),\n",
       " (':Þ', 1.1),\n",
       " (';)', 0.9),\n",
       " (';-)', 1.0),\n",
       " (';-*', 2.2),\n",
       " (';-]', 0.7),\n",
       " (';d', 0.8),\n",
       " (';D', 0.8),\n",
       " (';]', 0.6),\n",
       " (';^)', 1.4),\n",
       " ('</3', -3.0),\n",
       " ('<3', 1.9),\n",
       " ('<:', 2.1),\n",
       " ('<:-|', -1.4),\n",
       " ('=)', 2.2),\n",
       " ('=-3', 2.0),\n",
       " ('=-d', 2.4),\n",
       " ('=-D', 2.4),\n",
       " ('=/', -1.4),\n",
       " ('=3', 2.1),\n",
       " ('=d', 2.3),\n",
       " ('=D', 2.3),\n",
       " ('=l', -1.2),\n",
       " ('=\\\\', -1.2),\n",
       " ('=]', 1.6),\n",
       " ('=p', 1.3),\n",
       " ('=|', -0.8),\n",
       " ('>-:', -2.0),\n",
       " ('>.<', -1.3),\n",
       " ('>:', -2.1),\n",
       " ('>:(', -2.7),\n",
       " ('>:)', 0.4),\n",
       " ('>:-(', -2.7),\n",
       " ('>:-)', -0.4),\n",
       " ('>:/', -1.6),\n",
       " ('>:o', -1.2),\n",
       " ('>:p', 1.0),\n",
       " ('>:[', -2.1),\n",
       " ('>:\\\\', -1.7),\n",
       " ('>;(', -2.9),\n",
       " ('>;)', 0.1),\n",
       " ('>_>^', 2.1),\n",
       " ('@:', -2.1),\n",
       " ('@>-->--', 2.1),\n",
       " (\"@}-;-'---\", 2.2),\n",
       " ('aas', 2.5),\n",
       " ('aayf', 2.7),\n",
       " ('afu', -2.9),\n",
       " ('alol', 2.8),\n",
       " ('ambw', 2.9),\n",
       " ('aml', 3.4),\n",
       " ('atab', -1.9),\n",
       " ('awol', -1.3),\n",
       " ('ayc', 0.2),\n",
       " ('ayor', -1.2),\n",
       " ('aug-00', 0.3),\n",
       " ('bfd', -2.7),\n",
       " ('bfe', -2.6),\n",
       " ('bff', 2.9),\n",
       " ('bffn', 1.0),\n",
       " ('bl', 2.3),\n",
       " ('bsod', -2.2),\n",
       " ('btd', -2.1),\n",
       " ('btdt', -0.1),\n",
       " ('bz', 0.4),\n",
       " ('b^d', 2.6),\n",
       " ('cwot', -2.3),\n",
       " (\"d-':\", -2.5),\n",
       " ('d8', -3.2),\n",
       " ('d:', 1.2),\n",
       " ('d:<', -3.2),\n",
       " ('d;', -2.9),\n",
       " ('d=', 1.5),\n",
       " ('doa', -2.3),\n",
       " ('dx', -3.0),\n",
       " ('ez', 1.5),\n",
       " ('fav', 2.0),\n",
       " ('fcol', -1.8),\n",
       " ('ff', 1.8),\n",
       " ('ffs', -2.8),\n",
       " ('fkm', -2.4),\n",
       " ('foaf', 1.8),\n",
       " ('ftw', 2.0),\n",
       " ('fu', -3.7),\n",
       " ('fubar', -3.0),\n",
       " ('fwb', 2.5),\n",
       " ('fyi', 0.8),\n",
       " ('fysa', 0.4),\n",
       " ('g1', 1.4),\n",
       " ('gg', 1.2),\n",
       " ('gga', 1.7),\n",
       " ('gigo', -0.6),\n",
       " ('gj', 2.0),\n",
       " ('gl', 1.3),\n",
       " ('gla', 2.5),\n",
       " ('gn', 1.2),\n",
       " ('gr8', 2.7),\n",
       " ('grrr', -0.4),\n",
       " ('gt', 1.1),\n",
       " ('h&k', 2.3),\n",
       " ('hagd', 2.2),\n",
       " ('hagn', 2.2),\n",
       " ('hago', 1.2),\n",
       " ('hak', 1.9),\n",
       " ('hand', 2.2),\n",
       " ('hho1/2k', 1.4),\n",
       " ('hhoj', 2.0),\n",
       " ('hhok', 0.9),\n",
       " ('hugz', 2.0),\n",
       " ('hi5', 1.9),\n",
       " ('idk', -0.4),\n",
       " ('ijs', 0.7),\n",
       " ('ilu', 3.4),\n",
       " ('iluaaf', 2.7),\n",
       " ('ily', 3.4),\n",
       " ('ily2', 2.6),\n",
       " ('iou', 0.7),\n",
       " ('iyq', 2.3),\n",
       " ('j/j', 2.0),\n",
       " ('j/k', 1.6),\n",
       " ('j/p', 1.4),\n",
       " ('j/t', -0.2),\n",
       " ('j/w', 1.0),\n",
       " ('j4f', 1.4),\n",
       " ('j4g', 1.7),\n",
       " ('jho', 0.8),\n",
       " ('jhomf', 1.0),\n",
       " ('jj', 1.0),\n",
       " ('jk', 0.9),\n",
       " ('jp', 0.8),\n",
       " ('jt', 0.9),\n",
       " ('jw', 1.6),\n",
       " ('jealz', -1.2),\n",
       " ('k4y', 2.3),\n",
       " ('kfy', 2.3),\n",
       " ('kia', -3.2),\n",
       " ('kk', 1.5),\n",
       " ('kmuf', 2.2),\n",
       " ('l', 2.0),\n",
       " ('l&r', 2.2),\n",
       " ('laoj', 1.3),\n",
       " ('lmao', 2.9),\n",
       " ('lmbao', 1.8),\n",
       " ('lmfao', 2.5),\n",
       " ('lmso', 2.7),\n",
       " ('lol', 1.8),\n",
       " ('lolz', 2.7),\n",
       " ('lts', 1.6),\n",
       " ('ly', 2.6),\n",
       " ('ly4e', 2.7),\n",
       " ('lya', 3.3),\n",
       " ('lyb', 3.0),\n",
       " ('lyl', 3.1),\n",
       " ('lylab', 2.7),\n",
       " ('lylas', 2.6),\n",
       " ('lylb', 1.6),\n",
       " ('m8', 1.4),\n",
       " ('mia', -1.2),\n",
       " ('mml', 2.0),\n",
       " ('mofo', -2.4),\n",
       " ('muah', 2.3),\n",
       " ('mubar', -1.0),\n",
       " ('musm', 0.9),\n",
       " ('mwah', 2.5),\n",
       " ('n1', 1.9),\n",
       " ('nbd', 1.3),\n",
       " ('nbif', -0.5),\n",
       " ('nfc', -2.7),\n",
       " ('nfw', -2.4),\n",
       " ('nh', 2.2),\n",
       " ('nimby', -0.8),\n",
       " ('nimjd', -0.7),\n",
       " ('nimq', -0.2),\n",
       " ('nimy', -1.4),\n",
       " ('nitl', -1.5),\n",
       " ('nme', -2.1),\n",
       " ('noyb', -0.7),\n",
       " ('np', 1.4),\n",
       " ('ntmu', 1.4),\n",
       " ('o-8', -0.5),\n",
       " ('o-:', -0.3),\n",
       " ('o-|', -1.1),\n",
       " ('o.o', -0.8),\n",
       " ('O.o', -0.6),\n",
       " ('o.O', -0.6),\n",
       " ('o:', -0.2),\n",
       " ('o:)', 1.5),\n",
       " ('o:-)', 2.0),\n",
       " ('o:-3', 2.2),\n",
       " ('o:3', 2.3),\n",
       " ('o:<', -0.3),\n",
       " ('o;^)', 1.6),\n",
       " ('ok', 1.2),\n",
       " ('o_o', -0.5),\n",
       " ('O_o', -0.5),\n",
       " ('o_O', -0.5),\n",
       " ('pita', -2.4),\n",
       " ('pls', 0.3),\n",
       " ('plz', 0.3),\n",
       " ('pmbi', 0.8),\n",
       " ('pmfji', 0.3),\n",
       " ('pmji', 0.7),\n",
       " ('po', -2.6),\n",
       " ('ptl', 2.6),\n",
       " ('pu', -1.1),\n",
       " ('qq', -2.2),\n",
       " ('qt', 1.8),\n",
       " ('r&r', 2.4),\n",
       " ('rofl', 2.7),\n",
       " ('roflmao', 2.5),\n",
       " ('rotfl', 2.6),\n",
       " ('rotflmao', 2.8),\n",
       " ('rotflmfao', 2.5),\n",
       " ('rotflol', 3.0),\n",
       " ('rotgl', 2.9),\n",
       " ('rotglmao', 1.8),\n",
       " ('s:', -1.1),\n",
       " ('sapfu', -1.1),\n",
       " ('sete', 2.8),\n",
       " ('sfete', 2.7),\n",
       " ('sgtm', 2.4),\n",
       " ('slap', 0.6),\n",
       " ('slaw', 2.1),\n",
       " ('smh', -1.3),\n",
       " ('snafu', -2.5),\n",
       " ('sob', -1.0),\n",
       " ('swak', 2.3),\n",
       " ('tgif', 2.3),\n",
       " ('thks', 1.4),\n",
       " ('thx', 1.5),\n",
       " ('tia', 2.3),\n",
       " ('tmi', -0.3),\n",
       " ('tnx', 1.1),\n",
       " ('true', 1.8),\n",
       " ('tx', 1.5),\n",
       " ('txs', 1.1),\n",
       " ('ty', 1.6),\n",
       " ('tyvm', 2.5),\n",
       " ('urw', 1.9),\n",
       " ('vbg', 2.1),\n",
       " ('vbs', 3.1),\n",
       " ('vip', 2.3),\n",
       " ('vwd', 2.6),\n",
       " ('vwp', 2.1),\n",
       " ('wag', -0.2),\n",
       " ('wd', 2.7),\n",
       " ('wilco', 0.9),\n",
       " ('wp', 1.0),\n",
       " ('wtf', -2.8),\n",
       " ('wtg', 2.1),\n",
       " ('wth', -2.4),\n",
       " ('x-d', 2.6),\n",
       " ('x-p', 1.7),\n",
       " ('xd', 2.8),\n",
       " ('xlnt', 3.0),\n",
       " ('xoxo', 3.0),\n",
       " ('xoxozzz', 2.3),\n",
       " ('xp', 1.6),\n",
       " ('xqzt', 1.6),\n",
       " ('xtc', 0.8),\n",
       " ('yolo', 1.1),\n",
       " ('yoyo', 0.4),\n",
       " ('yvw', 1.6),\n",
       " ('yw', 1.8),\n",
       " ('ywia', 2.5),\n",
       " ('zzz', -1.2),\n",
       " ('[-;', 0.5),\n",
       " ('[:', 1.3),\n",
       " ('[;', 1.0),\n",
       " ('[=', 1.7),\n",
       " ('\\\\-:', -1.0),\n",
       " ('\\\\:', -1.0),\n",
       " ('\\\\:<', -1.7),\n",
       " ('\\\\=', -1.1),\n",
       " ('\\\\^:', -1.3),\n",
       " ('\\\\o/', 2.2),\n",
       " ('\\\\o:', -1.2),\n",
       " (']-:', -2.1),\n",
       " (']:', -1.6),\n",
       " (']:<', -2.5),\n",
       " ('^<_<', 1.4),\n",
       " ('^urs', -2.8),\n",
       " ('abandon', -1.9),\n",
       " ('abandoned', -2.0),\n",
       " ('abandoner', -1.9),\n",
       " ('abandoners', -1.9),\n",
       " ('abandoning', -1.6),\n",
       " ('abandonment', -2.4),\n",
       " ('abandonments', -1.7),\n",
       " ('abandons', -1.3),\n",
       " ('abducted', -2.3),\n",
       " ('abduction', -2.8),\n",
       " ('abductions', -2.0),\n",
       " ('abhor', -2.0),\n",
       " ('abhorred', -2.4),\n",
       " ('abhorrent', -3.1),\n",
       " ('abhors', -2.9),\n",
       " ('abilities', 1.0),\n",
       " ('ability', 1.3),\n",
       " ('aboard', 0.1),\n",
       " ('absentee', -1.1),\n",
       " ('absentees', -0.8),\n",
       " ('absolve', 1.2),\n",
       " ('absolved', 1.5),\n",
       " ('absolves', 1.3),\n",
       " ('absolving', 1.6),\n",
       " ('abuse', -3.2),\n",
       " ('abused', -2.3),\n",
       " ('abuser', -2.6),\n",
       " ('abusers', -2.6),\n",
       " ('abuses', -2.6),\n",
       " ('abusing', -2.0),\n",
       " ('abusive', -3.2),\n",
       " ('abusively', -2.8),\n",
       " ('abusiveness', -2.5),\n",
       " ('abusivenesses', -3.0),\n",
       " ('accept', 1.6),\n",
       " ('acceptabilities', 1.6),\n",
       " ('acceptability', 1.1),\n",
       " ('acceptable', 1.3),\n",
       " ('acceptableness', 1.3),\n",
       " ('acceptably', 1.5),\n",
       " ('acceptance', 2.0),\n",
       " ('acceptances', 1.7),\n",
       " ('acceptant', 1.6),\n",
       " ('acceptation', 1.3),\n",
       " ('acceptations', 0.9),\n",
       " ('accepted', 1.1),\n",
       " ('accepting', 1.6),\n",
       " ('accepts', 1.3),\n",
       " ('accident', -2.1),\n",
       " ('accidental', -0.3),\n",
       " ('accidentally', -1.4),\n",
       " ('accidents', -1.3),\n",
       " ('accomplish', 1.8),\n",
       " ('accomplished', 1.9),\n",
       " ('accomplishes', 1.7),\n",
       " ('accusation', -1.0),\n",
       " ('accusations', -1.3),\n",
       " ('accuse', -0.8),\n",
       " ('accused', -1.2),\n",
       " ('accuses', -1.4),\n",
       " ('accusing', -0.7),\n",
       " ('ache', -1.6),\n",
       " ('ached', -1.6),\n",
       " ('aches', -1.0),\n",
       " ('achievable', 1.3),\n",
       " ('aching', -2.2),\n",
       " ('acquit', 0.8),\n",
       " ('acquits', 0.1),\n",
       " ('acquitted', 1.0),\n",
       " ('acquitting', 1.3),\n",
       " ('acrimonious', -1.7),\n",
       " ('active', 1.7),\n",
       " ('actively', 1.3),\n",
       " ('activeness', 0.6),\n",
       " ('activenesses', 0.8),\n",
       " ('actives', 1.1),\n",
       " ('adequate', 0.9),\n",
       " ('admirability', 2.4),\n",
       " ('admirable', 2.6),\n",
       " ('admirableness', 2.2),\n",
       " ('admirably', 2.5),\n",
       " ('admiral', 1.3),\n",
       " ('admirals', 1.5),\n",
       " ('admiralties', 1.6),\n",
       " ('admiralty', 1.2),\n",
       " ('admiration', 2.5),\n",
       " ('admirations', 1.6),\n",
       " ('admire', 2.1),\n",
       " ('admired', 2.3),\n",
       " ('admirer', 1.8),\n",
       " ('admirers', 1.7),\n",
       " ('admires', 1.5),\n",
       " ('admiring', 1.6),\n",
       " ('admiringly', 2.3),\n",
       " ('admit', 0.8),\n",
       " ('admits', 1.2),\n",
       " ('admitted', 0.4),\n",
       " ('admonished', -1.9),\n",
       " ('adopt', 0.7),\n",
       " ('adopts', 0.7),\n",
       " ('adorability', 2.2),\n",
       " ('adorable', 2.2),\n",
       " ('adorableness', 2.5),\n",
       " ('adorably', 2.1),\n",
       " ('adoration', 2.9),\n",
       " ('adorations', 2.2),\n",
       " ('adore', 2.6),\n",
       " ('adored', 1.8),\n",
       " ('adorer', 1.7),\n",
       " ('adorers', 2.1),\n",
       " ('adores', 1.6),\n",
       " ('adoring', 2.6),\n",
       " ('adoringly', 2.4),\n",
       " ('adorn', 0.9),\n",
       " ('adorned', 0.8),\n",
       " ('adorner', 1.3),\n",
       " ('adorners', 0.9),\n",
       " ('adorning', 1.0),\n",
       " ('adornment', 1.3),\n",
       " ('adornments', 0.8),\n",
       " ('adorns', 0.5),\n",
       " ('advanced', 1.0),\n",
       " ('advantage', 1.0),\n",
       " ('advantaged', 1.4),\n",
       " ('advantageous', 1.5),\n",
       " ('advantageously', 1.9),\n",
       " ('advantageousness', 1.6),\n",
       " ('advantages', 1.5),\n",
       " ('advantaging', 1.6),\n",
       " ('adventure', 1.3),\n",
       " ('adventured', 1.3),\n",
       " ('adventurer', 1.2),\n",
       " ('adventurers', 0.9),\n",
       " ('adventures', 1.4),\n",
       " ('adventuresome', 1.7),\n",
       " ('adventuresomeness', 1.3),\n",
       " ('adventuress', 0.8),\n",
       " ('adventuresses', 1.4),\n",
       " ('adventuring', 2.3),\n",
       " ('adventurism', 1.5),\n",
       " ('adventurist', 1.4),\n",
       " ('adventuristic', 1.7),\n",
       " ('adventurists', 1.2),\n",
       " ('adventurous', 1.4),\n",
       " ('adventurously', 1.3),\n",
       " ('adventurousness', 1.8),\n",
       " ('adversarial', -1.5),\n",
       " ('adversaries', -1.0),\n",
       " ('adversary', -0.8),\n",
       " ('adversative', -1.2),\n",
       " ('adversatively', -0.1),\n",
       " ('adversatives', -1.0),\n",
       " ('adverse', -1.5),\n",
       " ('adversely', -0.8),\n",
       " ('adverseness', -0.6),\n",
       " ('adversities', -1.5),\n",
       " ('adversity', -1.8),\n",
       " ('affected', -0.6),\n",
       " ('affection', 2.4),\n",
       " ('affectional', 1.9),\n",
       " ('affectionally', 1.5),\n",
       " ('affectionate', 1.9),\n",
       " ('affectionately', 2.2),\n",
       " ('affectioned', 1.8),\n",
       " ('affectionless', -2.0),\n",
       " ('affections', 1.5),\n",
       " ('afflicted', -1.5),\n",
       " ('affronted', 0.2),\n",
       " ('aggravate', -2.5),\n",
       " ('aggravated', -1.9),\n",
       " ('aggravates', -1.9),\n",
       " ('aggravating', -1.2),\n",
       " ('aggress', -1.3),\n",
       " ('aggressed', -1.4),\n",
       " ('aggresses', -0.5),\n",
       " ('aggressing', -0.6),\n",
       " ('aggression', -1.2),\n",
       " ('aggressions', -1.3),\n",
       " ('aggressive', -0.6),\n",
       " ('aggressively', -1.3),\n",
       " ('aggressiveness', -1.8),\n",
       " ('aggressivities', -1.4),\n",
       " ('aggressivity', -0.6),\n",
       " ('aggressor', -0.8),\n",
       " ('aggressors', -0.9),\n",
       " ('aghast', -1.9),\n",
       " ('agitate', -1.7),\n",
       " ('agitated', -2.0),\n",
       " ('agitatedly', -1.6),\n",
       " ('agitates', -1.4),\n",
       " ('agitating', -1.8),\n",
       " ('agitation', -1.0),\n",
       " ('agitational', -1.2),\n",
       " ('agitations', -1.3),\n",
       " ('agitative', -1.3),\n",
       " ('agitato', -0.1),\n",
       " ('agitator', -1.4),\n",
       " ('agitators', -2.1),\n",
       " ('agog', 1.9),\n",
       " ('agonise', -2.1),\n",
       " ('agonised', -2.3),\n",
       " ('agonises', -2.4),\n",
       " ('agonising', -1.5),\n",
       " ('agonize', -2.3),\n",
       " ('agonized', -2.2),\n",
       " ('agonizes', -2.3),\n",
       " ('agonizing', -2.7),\n",
       " ('agonizingly', -2.3),\n",
       " ('agony', -1.8),\n",
       " ('agree', 1.5),\n",
       " ('agreeability', 1.9),\n",
       " ('agreeable', 1.8),\n",
       " ('agreeableness', 1.8),\n",
       " ('agreeablenesses', 1.3),\n",
       " ('agreeably', 1.6),\n",
       " ('agreed', 1.1),\n",
       " ('agreeing', 1.4),\n",
       " ('agreement', 2.2),\n",
       " ('agreements', 1.1),\n",
       " ('agrees', 0.8),\n",
       " ('alarm', -1.4),\n",
       " ('alarmed', -1.4),\n",
       " ('alarming', -0.5),\n",
       " ('alarmingly', -2.6),\n",
       " ('alarmism', -0.3),\n",
       " ('alarmists', -1.1),\n",
       " ('alarms', -1.1),\n",
       " ('alas', -1.1),\n",
       " ('alert', 1.2),\n",
       " ('alienation', -1.1),\n",
       " ('alive', 1.6),\n",
       " ('allergic', -1.2),\n",
       " ('allow', 0.9),\n",
       " ('alone', -1.0),\n",
       " ('alright', 1.0),\n",
       " ('amaze', 2.5),\n",
       " ('amazed', 2.2),\n",
       " ('amazedly', 2.1),\n",
       " ('amazement', 2.5),\n",
       " ('amazements', 2.2),\n",
       " ('amazes', 2.2),\n",
       " ('amazing', 2.8),\n",
       " ('amazon', 0.7),\n",
       " ('amazonite', 0.2),\n",
       " ('amazons', -0.1),\n",
       " ('amazonstone', 1.0),\n",
       " ('amazonstones', 0.2),\n",
       " ('ambitious', 2.1),\n",
       " ('ambivalent', 0.5),\n",
       " ('amor', 3.0),\n",
       " ('amoral', -1.6),\n",
       " ('amoralism', -0.7),\n",
       " ('amoralisms', -0.7),\n",
       " ('amoralities', -1.2),\n",
       " ('amorality', -1.5),\n",
       " ('amorally', -1.0),\n",
       " ('amoretti', 0.2),\n",
       " ('amoretto', 0.6),\n",
       " ('amorettos', 0.3),\n",
       " ('amorino', 1.2),\n",
       " ('amorist', 1.6),\n",
       " ('amoristic', 1.0),\n",
       " ('amorists', 0.1),\n",
       " ('amoroso', 2.3),\n",
       " ('amorous', 1.8),\n",
       " ('amorously', 2.3),\n",
       " ('amorousness', 2.0),\n",
       " ('amorphous', -0.2),\n",
       " ('amorphously', 0.1),\n",
       " ('amorphousness', 0.3),\n",
       " ('amort', -2.1),\n",
       " ('amortise', 0.5),\n",
       " ('amortised', -0.2),\n",
       " ('amortises', 0.1),\n",
       " ('amortizable', 0.5),\n",
       " ('amortization', 0.6),\n",
       " ('amortizations', 0.2),\n",
       " ('amortize', -0.1),\n",
       " ('amortized', 0.8),\n",
       " ('amortizes', 0.6),\n",
       " ('amortizing', 0.8),\n",
       " ('amusable', 0.7),\n",
       " ('amuse', 1.7),\n",
       " ('amused', 1.8),\n",
       " ('amusedly', 2.2),\n",
       " ('amusement', 1.5),\n",
       " ('amusements', 1.5),\n",
       " ('amuser', 1.1),\n",
       " ('amusers', 1.3),\n",
       " ('amuses', 1.7),\n",
       " ('amusia', 0.3),\n",
       " ('amusias', -0.4),\n",
       " ('amusing', 1.6),\n",
       " ('amusingly', 0.8),\n",
       " ('amusingness', 1.8),\n",
       " ('amusive', 1.7),\n",
       " ('anger', -2.7),\n",
       " ('angered', -2.3),\n",
       " ('angering', -2.2),\n",
       " ('angerly', -1.9),\n",
       " ('angers', -2.3),\n",
       " ('angrier', -2.3),\n",
       " ('angriest', -3.1),\n",
       " ('angrily', -1.8),\n",
       " ('angriness', -1.7),\n",
       " ('angry', -2.3),\n",
       " ('anguish', -2.9),\n",
       " ('anguished', -1.8),\n",
       " ('anguishes', -2.1),\n",
       " ('anguishing', -2.7),\n",
       " ('animosity', -1.9),\n",
       " ('annoy', -1.9),\n",
       " ('annoyance', -1.3),\n",
       " ('annoyances', -1.8),\n",
       " ('annoyed', -1.6),\n",
       " ('annoyer', -2.2),\n",
       " ('annoyers', -1.5),\n",
       " ('annoying', -1.7),\n",
       " ('annoys', -1.8),\n",
       " ('antagonism', -1.9),\n",
       " ('antagonisms', -1.2),\n",
       " ('antagonist', -1.9),\n",
       " ('antagonistic', -1.7),\n",
       " ('antagonistically', -2.2),\n",
       " ('antagonists', -1.7),\n",
       " ('antagonize', -2.0),\n",
       " ('antagonized', -1.4),\n",
       " ('antagonizes', -0.5),\n",
       " ('antagonizing', -2.7),\n",
       " ('anti', -1.3),\n",
       " ('anticipation', 0.4),\n",
       " ('anxieties', -0.6),\n",
       " ('anxiety', -0.7),\n",
       " ('anxious', -1.0),\n",
       " ('anxiously', -0.9),\n",
       " ('anxiousness', -1.0),\n",
       " ('aok', 2.0),\n",
       " ('apathetic', -1.2),\n",
       " ('apathetically', -0.4),\n",
       " ('apathies', -0.6),\n",
       " ('apathy', -1.2),\n",
       " ('apeshit', -0.9),\n",
       " ('apocalyptic', -3.4),\n",
       " ('apologise', 1.6),\n",
       " ('apologised', 0.4),\n",
       " ('apologises', 0.8),\n",
       " ('apologising', 0.2),\n",
       " ('apologize', 0.4),\n",
       " ('apologized', 1.3),\n",
       " ('apologizes', 1.5),\n",
       " ('apologizing', -0.3),\n",
       " ('apology', 0.2),\n",
       " ('appall', -2.4),\n",
       " ('appalled', -2.0),\n",
       " ('appalling', -1.5),\n",
       " ('appallingly', -2.0),\n",
       " ('appalls', -1.9),\n",
       " ('appease', 1.1),\n",
       " ('appeased', 0.9),\n",
       " ('appeases', 0.9),\n",
       " ('appeasing', 1.0),\n",
       " ('applaud', 2.0),\n",
       " ('applauded', 1.5),\n",
       " ('applauding', 2.1),\n",
       " ('applauds', 1.4),\n",
       " ('applause', 1.8),\n",
       " ('appreciate', 1.7),\n",
       " ('appreciated', 2.3),\n",
       " ('appreciates', 2.3),\n",
       " ('appreciating', 1.9),\n",
       " ('appreciation', 2.3),\n",
       " ('appreciations', 1.7),\n",
       " ('appreciative', 2.6),\n",
       " ('appreciatively', 1.8),\n",
       " ('appreciativeness', 1.6),\n",
       " ('appreciator', 2.6),\n",
       " ('appreciators', 1.5),\n",
       " ('appreciatory', 1.7),\n",
       " ('apprehensible', 1.1),\n",
       " ('apprehensibly', -0.2),\n",
       " ('apprehension', -2.1),\n",
       " ('apprehensions', -0.9),\n",
       " ('apprehensively', -0.3),\n",
       " ('apprehensiveness', -0.7),\n",
       " ('approval', 2.1),\n",
       " ('approved', 1.8),\n",
       " ('approves', 1.7),\n",
       " ('ardent', 2.1),\n",
       " ('arguable', -1.0),\n",
       " ('arguably', -1.0),\n",
       " ('argue', -1.4),\n",
       " ('argued', -1.5),\n",
       " ('arguer', -1.6),\n",
       " ('arguers', -1.4),\n",
       " ('argues', -1.6),\n",
       " ('arguing', -2.0),\n",
       " ('argument', -1.5),\n",
       " ('argumentative', -1.5),\n",
       " ('argumentatively', -1.8),\n",
       " ('argumentive', -1.5),\n",
       " ('arguments', -1.7),\n",
       " ('arrest', -1.4),\n",
       " ('arrested', -2.1),\n",
       " ('arrests', -1.9),\n",
       " ('arrogance', -2.4),\n",
       " ('arrogances', -1.9),\n",
       " ('arrogant', -2.2),\n",
       " ('arrogantly', -1.8),\n",
       " ('ashamed', -2.1),\n",
       " ('ashamedly', -1.7),\n",
       " ('ass', -2.5),\n",
       " ('assassination', -2.9),\n",
       " ('assassinations', -2.7),\n",
       " ('assault', -2.8),\n",
       " ('assaulted', -2.4),\n",
       " ('assaulting', -2.3),\n",
       " ('assaultive', -2.8),\n",
       " ('assaults', -2.5),\n",
       " ('asset', 1.5),\n",
       " ('assets', 0.7),\n",
       " ('assfucking', -2.5),\n",
       " ('assholes', -2.8),\n",
       " ('assurance', 1.4),\n",
       " ('assurances', 1.4),\n",
       " ('assure', 1.4),\n",
       " ('assured', 1.5),\n",
       " ('assuredly', 1.6),\n",
       " ('assuredness', 1.4),\n",
       " ('assurer', 0.9),\n",
       " ('assurers', 1.1),\n",
       " ('assures', 1.3),\n",
       " ('assurgent', 1.3),\n",
       " ('assuring', 1.6),\n",
       " ('assuror', 0.5),\n",
       " ('assurors', 0.7),\n",
       " ('astonished', 1.6),\n",
       " ('astound', 1.7),\n",
       " ('astounded', 1.8),\n",
       " ('astounding', 1.8),\n",
       " ('astoundingly', 2.1),\n",
       " ('astounds', 2.1),\n",
       " ('attachment', 1.2),\n",
       " ('attachments', 1.1),\n",
       " ('attack', -2.1),\n",
       " ('attacked', -2.0),\n",
       " ('attacker', -2.7),\n",
       " ('attackers', -2.7),\n",
       " ('attacking', -2.0),\n",
       " ('attacks', -1.9),\n",
       " ('attract', 1.5),\n",
       " ('attractancy', 0.9),\n",
       " ('attractant', 1.3),\n",
       " ('attractants', 1.4),\n",
       " ('attracted', 1.8),\n",
       " ('attracting', 2.1),\n",
       " ('attraction', 2.0),\n",
       " ('attractions', 1.8),\n",
       " ('attractive', 1.9),\n",
       " ('attractively', 2.2),\n",
       " ('attractiveness', 1.8),\n",
       " ('attractivenesses', 2.1),\n",
       " ('attractor', 1.2),\n",
       " ('attractors', 1.2),\n",
       " ('attracts', 1.7),\n",
       " ('audacious', 0.9),\n",
       " ('authority', 0.3),\n",
       " ('aversion', -1.9),\n",
       " ('aversions', -1.1),\n",
       " ('aversive', -1.6),\n",
       " ('aversively', -0.8),\n",
       " ('avert', -0.7),\n",
       " ('averted', -0.3),\n",
       " ('averts', -0.4),\n",
       " ('avid', 1.2),\n",
       " ('avoid', -1.2),\n",
       " ('avoidance', -1.7),\n",
       " ('avoidances', -1.1),\n",
       " ('avoided', -1.4),\n",
       " ('avoider', -1.8),\n",
       " ('avoiders', -1.4),\n",
       " ('avoiding', -1.4),\n",
       " ('avoids', -0.7),\n",
       " ('await', 0.4),\n",
       " ('awaited', -0.1),\n",
       " ('awaits', 0.3),\n",
       " ('award', 2.5),\n",
       " ('awardable', 2.4),\n",
       " ('awarded', 1.7),\n",
       " ('awardee', 1.8),\n",
       " ('awardees', 1.2),\n",
       " ('awarder', 0.9),\n",
       " ('awarders', 1.3),\n",
       " ('awarding', 1.9),\n",
       " ('awards', 2.0),\n",
       " ('awesome', 3.1),\n",
       " ('awful', -2.0),\n",
       " ('awkward', -0.6),\n",
       " ('awkwardly', -1.3),\n",
       " ('awkwardness', -0.7),\n",
       " ('axe', -0.4),\n",
       " ('axed', -1.3),\n",
       " ('backed', 0.1),\n",
       " ('backing', 0.1),\n",
       " ('backs', -0.2),\n",
       " ('bad', -2.5),\n",
       " ('badass', -0.6),\n",
       " ('badly', -2.1),\n",
       " ('bailout', -0.4),\n",
       " ('bamboozle', -1.5),\n",
       " ('bamboozled', -1.5),\n",
       " ('bamboozles', -1.5),\n",
       " ('ban', -2.6),\n",
       " ('banish', -1.9),\n",
       " ('bankrupt', -2.6),\n",
       " ('bankster', -2.1),\n",
       " ('banned', -2.0),\n",
       " ('bargain', 0.8),\n",
       " ('barrier', -0.5),\n",
       " ('bashful', -0.1),\n",
       " ('bashfully', 0.2),\n",
       " ('bashfulness', -0.8),\n",
       " ('bastard', -2.5),\n",
       " ('bastardies', -1.8),\n",
       " ('bastardise', -2.1),\n",
       " ('bastardised', -2.3),\n",
       " ('bastardises', -2.3),\n",
       " ('bastardising', -2.6),\n",
       " ('bastardization', -2.4),\n",
       " ('bastardizations', -2.1),\n",
       " ('bastardize', -2.4),\n",
       " ('bastardized', -2.0),\n",
       " ('bastardizes', -1.8),\n",
       " ('bastardizing', -2.3),\n",
       " ('bastardly', -2.7),\n",
       " ('bastards', -3.0),\n",
       " ('bastardy', -2.7),\n",
       " ('battle', -1.6),\n",
       " ('battled', -1.2),\n",
       " ('battlefield', -1.6),\n",
       " ('battlefields', -0.9),\n",
       " ('battlefront', -1.2),\n",
       " ('battlefronts', -0.8),\n",
       " ('battleground', -1.7),\n",
       " ('battlegrounds', -0.6),\n",
       " ('battlement', -0.4),\n",
       " ('battlements', -0.4),\n",
       " ('battler', -0.8),\n",
       " ('battlers', -0.2),\n",
       " ('battles', -1.6),\n",
       " ('battleship', -0.1),\n",
       " ('battleships', -0.5),\n",
       " ('battlewagon', -0.3),\n",
       " ('battlewagons', -0.5),\n",
       " ('battling', -1.1),\n",
       " ('beaten', -1.8),\n",
       " ('beatific', 1.8),\n",
       " ('beating', -2.0),\n",
       " ('beaut', 1.6),\n",
       " ('beauteous', 2.5),\n",
       " ('beauteously', 2.6),\n",
       " ('beauteousness', 2.7),\n",
       " ('beautician', 1.2),\n",
       " ('beauticians', 0.4),\n",
       " ...]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tok,score) for tok,score in sa.lexicon.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e8c88594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.193, 'neu': 0.533, 'pos': 0.273, 'compound': 0.296}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.polarity_scores(\"Python is very readable and it's great for NLP. :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4a66b375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.711, 'pos': 0.289, 'compound': 0.431}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.polarity_scores(\"Python is not a bad choice for most applications.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ac3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis (Naive Bayes)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "movies = pd.read_csv(r\"C:\\Users\\91866\\Downloads\\hutto_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba649a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>Rock</th>\n",
       "      <th>is</th>\n",
       "      <th>destined</th>\n",
       "      <th>to</th>\n",
       "      <th>be</th>\n",
       "      <th>the</th>\n",
       "      <th>21st</th>\n",
       "      <th>Century's</th>\n",
       "      <th>new</th>\n",
       "      <th>...</th>\n",
       "      <th>Ill</th>\n",
       "      <th>slummer</th>\n",
       "      <th>Rashomon</th>\n",
       "      <th>dipsticks</th>\n",
       "      <th>Bearable</th>\n",
       "      <th>Staggeringly</th>\n",
       "      <th>’</th>\n",
       "      <th>ve</th>\n",
       "      <th>muttering</th>\n",
       "      <th>dissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10605 rows × 20756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       The  Rock  is  destined  to  be  the  21st  Century's  new  ...  Ill  \\\n",
       "0        1     1   1         1   2   1    1     1          1    1  ...    0   \n",
       "1        2     0   1         0   0   0    1     0          0    0  ...    0   \n",
       "2        0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "3        0     0   1         0   4   0    1     0          0    0  ...    0   \n",
       "4        0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "...    ...   ...  ..       ...  ..  ..  ...   ...        ...  ...  ...  ...   \n",
       "10600    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "10601    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "10602    0     0   0         0   0   0    0     0          0    0  ...    0   \n",
       "10603    0     0   0         0   0   0    2     0          0    0  ...    0   \n",
       "10604    0     0   0         0   0   0    2     0          0    0  ...    0   \n",
       "\n",
       "       slummer  Rashomon  dipsticks  Bearable  Staggeringly  ’  ve  muttering  \\\n",
       "0            0         0          0         0             0  0   0          0   \n",
       "1            0         0          0         0             0  0   0          0   \n",
       "2            0         0          0         0             0  0   0          0   \n",
       "3            0         0          0         0             0  0   0          0   \n",
       "4            0         0          0         0             0  0   0          0   \n",
       "...        ...       ...        ...       ...           ... ..  ..        ...   \n",
       "10600        0         0          0         0             0  0   0          0   \n",
       "10601        0         0          0         0             0  0   0          0   \n",
       "10602        0         0          0         0             0  0   0          0   \n",
       "10603        0         0          0         0             0  2   1          0   \n",
       "10604        0         0          0         0             0  0   0          1   \n",
       "\n",
       "       dissing  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "10600        0  \n",
       "10601        0  \n",
       "10602        0  \n",
       "10603        0  \n",
       "10604        1  \n",
       "\n",
       "[10605 rows x 20756 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import casual_tokenize\n",
    "bag_of_words = []\n",
    "from collections import Counter\n",
    "for text in movies['text']:\n",
    "    bag_of_words.append(Counter(casual_tokenize(text)))\n",
    "df_bows = pd.DataFrame.from_records(bag_of_words)\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "df_bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3039c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C\n",
       "0  1.0  NaN  NaN\n",
       "1  NaN  2.0  NaN\n",
       "2  NaN  NaN  3.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(list(({'A':1},{'B':2},{'C':3})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92a24fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB, MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(df_bows,movies.sentiment>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c209580",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['predicted_sentiment'] = model.predict_proba(df_bows)[:,1]*8-4\n",
    "movies['is_positive_original'] = (movies['sentiment']>0).astype(int)\n",
    "movies['is_positive_predicted'] = (movies['predicted_sentiment']>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdd3a09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>is_positive_original</th>\n",
       "      <th>is_positive_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
       "      <td>2.511515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.533333</td>\n",
       "      <td>The gorgeously elaborate continuation of ''The...</td>\n",
       "      <td>3.999904</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>Effective but too tepid biopic</td>\n",
       "      <td>-3.655976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>1.940954</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>Emerges as something rare, an issue movie that...</td>\n",
       "      <td>3.910373</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>10601</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>Well made but mush hearted.</td>\n",
       "      <td>-3.166489</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>10602</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>A real snooze.</td>\n",
       "      <td>-1.056805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>10603</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>No surprises.</td>\n",
       "      <td>-1.481449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>10604</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>We’ve seen the hippie turned yuppie plot befor...</td>\n",
       "      <td>3.988988</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>10605</td>\n",
       "      <td>-1.812500</td>\n",
       "      <td>Her fans walked out muttering words like ''hor...</td>\n",
       "      <td>-3.997954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10605 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sentiment                                               text  \\\n",
       "0          1   2.266667  The Rock is destined to be the 21st Century's ...   \n",
       "1          2   3.533333  The gorgeously elaborate continuation of ''The...   \n",
       "2          3  -0.600000                     Effective but too tepid biopic   \n",
       "3          4   1.466667  If you sometimes like to go to the movies to h...   \n",
       "4          5   1.733333  Emerges as something rare, an issue movie that...   \n",
       "...      ...        ...                                                ...   \n",
       "10600  10601  -0.062500                        Well made but mush hearted.   \n",
       "10601  10602  -1.500000                                     A real snooze.   \n",
       "10602  10603  -0.625000                                      No surprises.   \n",
       "10603  10604   1.437500  We’ve seen the hippie turned yuppie plot befor...   \n",
       "10604  10605  -1.812500  Her fans walked out muttering words like ''hor...   \n",
       "\n",
       "       predicted_sentiment  is_positive_original  is_positive_predicted  \n",
       "0                 2.511515                     1                      1  \n",
       "1                 3.999904                     1                      1  \n",
       "2                -3.655976                     0                      0  \n",
       "3                 1.940954                     1                      1  \n",
       "4                 3.910373                     1                      1  \n",
       "...                    ...                   ...                    ...  \n",
       "10600            -3.166489                     0                      0  \n",
       "10601            -1.056805                     0                      0  \n",
       "10602            -1.481449                     0                      0  \n",
       "10603             3.988988                     1                      1  \n",
       "10604            -3.997954                     0                      0  \n",
       "\n",
       "[10605 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "226e681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      5363\n",
      "           1       0.94      0.92      0.93      5242\n",
      "\n",
      "    accuracy                           0.93     10605\n",
      "   macro avg       0.93      0.93      0.93     10605\n",
      "weighted avg       0.93      0.93      0.93     10605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(movies['is_positive_original'],movies['is_positive_predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "486c7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry,\n",
    "the faster, would get home.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1147c964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "token = tokenizer.tokenize(sentence)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c86f7d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 1,\n",
       "         'faster': 3,\n",
       "         'Harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'the': 3,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(token)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4080c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faster', 3), ('the', 3), (',', 3), ('Harry', 2)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c0ab62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_harry_appears = bag_of_words['Harry']\n",
    "num_unique_words = len(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11dd507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears/num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf0d956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. A kite's wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite.\\n\\nThe lift that sustains the kite in flight is generated when air flows around the kite's surface, producing low pressure above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (e.g., the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle).\\n\\nThe same principles of fluid flow apply in liquids and kites are also used under water.\\n\\nA hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon.\\n\\nKites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have been made.\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "file_path = open(r\"C:\\Users\\91866\\Downloads\\kite_text.txt\",'r')\n",
    "with file_path as file:\n",
    "    x = file.read()\n",
    "x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8747aff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 16,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'as': 5,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'such': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(x.lower())\n",
    "token_counts = Counter(tokens)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6c1a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords',quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe3c4aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 16,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31577a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207,\n",
       " 0.06756756756756757,\n",
       " 0.036036036036036036,\n",
       " 0.02252252252252252,\n",
       " 0.018018018018018018,\n",
       " 0.018018018018018018,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordered collection\n",
    "\n",
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key,value in kite_counts.most_common():\n",
    "    document_vector.append(value/doc_length)\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c2a9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "779ba80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b891b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "len(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9db24f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[',',\n",
       "  '.',\n",
       "  'and',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'get',\n",
       "  'got',\n",
       "  'harry',\n",
       "  'harry',\n",
       "  'home',\n",
       "  'store',\n",
       "  'the',\n",
       "  'the',\n",
       "  'the',\n",
       "  'to',\n",
       "  'would'],\n",
       " ['.', 'and', 'faster', 'hairy', 'harry', 'is', 'jill', 'than'],\n",
       " ['.', 'as', 'as', 'hairy', 'harry', 'is', 'jill', 'not']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5986fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'harry',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'store',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'than',\n",
       " '.',\n",
       " 'as',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens,[])\n",
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "857bd856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89f5031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token,0)for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0651dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value/len(lexicon)\n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b40051f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "460008b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\91866\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9d721a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d174970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = [x.lower() for x in brown.words() if x not in puncs]\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33ad4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Intro tokens: 363\n",
      "Length of Intro tokens: 297\n"
     ]
    }
   ],
   "source": [
    "# IDF\n",
    "# Read the intro file\n",
    "file = open(r\"c:/Users/91866/Downloads/kite_text.txt\",'r')\n",
    "with file:\n",
    "    kite_intro = file.read()\n",
    "# Read the history file\n",
    "file = open(r\"c:/Users/91866/Downloads/kite_history.txt\")\n",
    "with file:\n",
    "    kite_history = file.read()\n",
    "\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "\n",
    "intro_total = len(intro_tokens)\n",
    "print(f'Length of Intro tokens: {intro_total}')\n",
    "\n",
    "history_total = len(history_tokens)\n",
    "print(f'Length of Intro tokens: {history_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3804786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kite': 0.0440771349862259}\n",
      "{'kite': 0.020202020202020204}\n"
     ]
    }
   ],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "history_counts = Counter(history_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite']/intro_total\n",
    "history_tf['kite'] = history_counts['kite']/history_total\n",
    "print(intro_tf)\n",
    "print(history_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "31d7011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0.027548209366391185}\n",
      "{'and': 0.030303030303030304}\n"
     ]
    }
   ],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "history_counts = Counter(history_tokens)\n",
    "intro_tf['and'] = intro_counts['and']/intro_total\n",
    "history_tf['and'] = history_counts['and']/history_total\n",
    "print(intro_tf)\n",
    "print(history_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "983f448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance ranking\n",
    "document_tfidf = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize((doc.lower()))\n",
    "    token_counts = Counter(tokens)\n",
    "    for key,value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key+=1\n",
    "        tf = value/len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs)/docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf*idf\n",
    "    document_tfidf.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb3a8c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0.05555555555555555),\n",
       "             ('and', 0),\n",
       "             ('as', 0.1111111111111111),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0.08333333333333333),\n",
       "             ('harry', 0.0),\n",
       "             ('home', 0),\n",
       "             ('is', 0.08333333333333333),\n",
       "             ('jill', 0.0),\n",
       "             ('not', 0.16666666666666666),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80f2c77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF with sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs \n",
    "vectorizer = TfidfVectorizer()\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8a6ed29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1614879 , 0.        , 0.48446369, 0.21233718, 0.21233718,\n",
       "        0.        , 0.25081952, 0.21233718, 0.        , 0.        ,\n",
       "        0.        , 0.21233718, 0.        , 0.63701154, 0.21233718,\n",
       "        0.21233718],\n",
       "       [0.36930805, 0.        , 0.36930805, 0.        , 0.        ,\n",
       "        0.36930805, 0.28680065, 0.        , 0.36930805, 0.36930805,\n",
       "        0.        , 0.        , 0.48559571, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.75143242, 0.        , 0.        , 0.        ,\n",
       "        0.28574186, 0.22190405, 0.        , 0.28574186, 0.28574186,\n",
       "        0.37571621, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "49fba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = {}\n",
    "\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(),np.random.rand(6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7e4791aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0.8928601514360016,\n",
       " 'dog': 0.3319798053011772,\n",
       " 'apple': 0.8212291230578318,\n",
       " 'lion': 0.0416966257252499,\n",
       " 'NYC': 0.10765667993596795,\n",
       " 'love': 0.5950520642062402}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1fbb0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['petness'] = (.3 * tfidf['cat']+\\\n",
    "                   .3 * tfidf['dog']+\\\n",
    "                   0 * tfidf['apple']+\\\n",
    "                   0 * tfidf['lion']-\\\n",
    "                   .2 * tfidf['NYC']+\\\n",
    "                   0.2 * tfidf['love'])\n",
    "topic['animalness'] = (.1 * tfidf['cat'] +\\\n",
    "                        .1 * tfidf['dog'] -\\\n",
    "                        .1 * tfidf['apple'] +\\\n",
    "                        .5 * tfidf['lion'] +\\\n",
    "                        .1 * tfidf['NYC'] -\\\n",
    "                        .1 * tfidf['love'])\n",
    "topic['cityness'] = ( 0 * tfidf['cat'] -\\\n",
    "                     .1 * tfidf['dog'] +\\\n",
    "                    .2 * tfidf['apple'] -\\\n",
    "                    .1 * tfidf['lion'] +\\\n",
    "                    .5 * tfidf['NYC'] +\\\n",
    "                    .1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "63ce9650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petness': 0.4649310638752081,\n",
       " 'animalness': 0.012469857803532436,\n",
       " 'cityness': 0.24021172789753165}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72f6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
